{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from spiking_models import DenseRNN, SpikingReLU, SpikingSigmoid, SpikingTanh, Accumulate\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(model, weights, x_test, y_test):\n",
    "    print(\"Converted model:\\n\" + \"-\"*32)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            print(\"Input Layer\")\n",
    "            inputs = tf.keras.Input(shape=(1, model.layers[0].input_shape[0][1]), batch_size=y_test.shape[0])\n",
    "            x = inputs\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            x = tf.keras.layers.Dense(layer.output_shape[1])(x)\n",
    "            # x = tf.keras.layers.RNN(DenseRNN(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            if layer.activation.__name__ == 'linear':\n",
    "                print(\"Dense Layer w/o activation\")\n",
    "                pass\n",
    "            elif layer.activation.__name__ == 'relu':\n",
    "                print(\"Dense Layer with SpikingReLU\")\n",
    "                x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'sigmoid':\n",
    "                print(\"Dense Layer with SpikingSigmoid\")\n",
    "                x = tf.keras.layers.RNN(SpikingSigmoid(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'tanh':\n",
    "                print(\"Dense Layer with SpikingTanh\")\n",
    "                x = tf.keras.layers.RNN(SpikingTanh(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            else:\n",
    "                print('[Info] Activation type', layer.activation.__name__, 'not implemented')\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            print(\"SpikingReLU Layer\")\n",
    "            x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "        elif isinstance(layer, tf.keras.layers.Softmax):\n",
    "            print(\"Accumulate + Softmax Layer\")\n",
    "            print(layer.output_shape[1])\n",
    "            x = tf.keras.layers.RNN(Accumulate(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            x = tf.keras.layers.Softmax()(x)\n",
    "        else:\n",
    "            print(\"[Info] Layer type \", layer, \"not implemented\")\n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "\n",
    "    spiking.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],)\n",
    "\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_weights(model, x_test, percentile=100):\n",
    "    max_activation = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.ReLU):\n",
    "            activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "            if np.percentile(activation, percentile) > max_activation:\n",
    "                max_activation = np.percentile(activation, percentile)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            if layer.activation.__name__ == 'relu':\n",
    "                activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "                if np.percentile(activation, percentile) > max_activation:\n",
    "                    max_activation = np.percentile(activation, percentile)\n",
    "\n",
    "    weights = model.get_weights()\n",
    "    if max_activation == 0:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNo normalization\\n\" + \"-\"*32)\n",
    "    else:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNormalizing by\", max_activation, \"\\n\" + \"-\"*32)\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] /= (max_activation)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def evaluate_conversion(converted_model, original_model, x_test, y_test, testacc, timesteps=50):\n",
    "    for i in range(1, timesteps+1):\n",
    "        _, acc = converted_model.evaluate(x_test, y_test, batch_size=y_test.shape[0], verbose=0)\n",
    "        print(\n",
    "            \"Timesteps\", str(i) + \"/\" + str(timesteps) + \" -\",\n",
    "            \"acc spiking (orig): %.2f%% (%.2f%%)\" % (acc*100, testacc*100),\n",
    "            \"- conv loss: %+.2f%%\" % ((-(1 - acc/testacc)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_conversion_model(weights, bias):\n",
    "    \"\"\"\n",
    "    Simple model-based conversion model proposed by Diehl et al.\n",
    "    :param weights: weights of the network.\n",
    "    :param bias: bias of the network.\n",
    "    :return: rescaled weights.\n",
    "    \"\"\"\n",
    "    # Get weights from trained network\n",
    "    converted_weights = weights\n",
    "    converted_bias = bias\n",
    "\n",
    "    # model based normalization\n",
    "    previous_factor = 1\n",
    "    for l in range(len(converted_weights)):\n",
    "        max_pos_input = 0\n",
    "        # Find maximum input for this layer\n",
    "        for o in range(converted_weights[l].shape[0]):\n",
    "            input_sum = 0\n",
    "            for i in range(converted_weights[l].shape[1]):\n",
    "                input_sum += tf.math.maximum(0, converted_weights[l][o, i])\n",
    "            if converted_bias is not None and converted_bias[l] is not None:\n",
    "                input_sum += tf.math.maximum(0, converted_bias[l][o])\n",
    "            max_pos_input = tf.math.maximum(max_pos_input, input_sum)\n",
    "\n",
    "        # get the maximum weight in the layer, in case all weights are negative, max_pos_input would be zero, so we\n",
    "        # use the max weight to rescale instead\n",
    "        max_wt = tf.math.reduce_max(converted_weights[l])\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            max_bias = tf.math.reduce_max(converted_bias[l])\n",
    "            max_wt = tf.math.maximum(max_wt, max_bias)\n",
    "        scale_factor = tf.math.maximum(max_wt, max_pos_input)\n",
    "        # Rescale all weights\n",
    "        applied_factor = scale_factor / previous_factor\n",
    "        converted_weights[l] = converted_weights[l] / applied_factor\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            converted_bias[l] = converted_bias[l] / scale_factor\n",
    "        previous_factor = scale_factor\n",
    "        print(f\"Scale factor for this layer is {previous_factor}\")\n",
    "\n",
    "    return converted_weights, converted_bias\n",
    "\n",
    "\n",
    "def weight_conversion_robust_and_data_based(weights, bias, model, data, normalization_method='robust',\n",
    "                                            ppercentile=0.99):\n",
    "\n",
    "    \"\"\"\n",
    "    Two methods proposed by Diehl et al and Rueckauer et al. Both methods are data-based, so they use weights and\n",
    "    activations to find the best scaling factor.\n",
    "    :param weights: weights of the network.\n",
    "    :param bias: bias of the network.\n",
    "    :param model: ann model.\n",
    "    :param data: dataset to determine activations.\n",
    "    :param normalization_method: type of normalization - robust (Rueckauer) or data (Diehl).\n",
    "    :param ppercentile: percentile of the activation, which is taken from maximal activation.\n",
    "    :return: rescaled weights.\n",
    "    \"\"\"\n",
    "    if normalization_method == 'data':\n",
    "        ppercentile = 1.0\n",
    "\n",
    "    # Get weights from trained network\n",
    "    converted_weights = weights\n",
    "    converted_bias = bias\n",
    "\n",
    "    # use training set to find max_act for each neuron\n",
    "\n",
    "    activations = []\n",
    "    for l in range(0, len(converted_weights)):\n",
    "        activation = get_activations_layer(model.input, model.layers[l].output, data)\n",
    "        activation_per_neuron = [np.max(activation[:, i]) for i in range(activation.shape[1])]\n",
    "        activations.append(activation_per_neuron)\n",
    "\n",
    "    previous_factor = 1\n",
    "    for l in range(len(converted_weights)):\n",
    "        # get the p-percentile of the activation\n",
    "        pos_inputs = activations[l]\n",
    "        pos_inputs.sort()\n",
    "        max_act = pos_inputs[int(ppercentile * (len(pos_inputs) - 1))]\n",
    "        # get the maximum weight in the layer\n",
    "        max_wt = tf.math.reduce_max(converted_weights[l])\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            max_bias = tf.math.reduce_max(converted_bias[l])\n",
    "            max_wt = tf.math.maximum(max_wt, max_bias)\n",
    "        scale_factor = tf.math.maximum(max_wt, max_act)\n",
    "\n",
    "        applied_factor = scale_factor / previous_factor\n",
    "        # rescale weights\n",
    "        converted_weights[l] = converted_weights[l] / applied_factor\n",
    "\n",
    "        # rescale bias\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            converted_bias[l] = converted_bias[l] / scale_factor\n",
    "        previous_factor = scale_factor\n",
    "        print(f\"Scale factor for this layer is {previous_factor}\")\n",
    "\n",
    "    return converted_weights, converted_bias\n",
    "\n",
    "\n",
    "def get_activations_layer(layer_in, layer_out, data, batch_size=32):\n",
    "\n",
    "    \"\"\"\n",
    "    Getting activation for specific layer of neural network.\n",
    "    :param layer_in: input layer of a model. For sequential models first layer, for functional model.layers[0].input can\n",
    "    be used.\n",
    "    :param layer_out: layer for which activations should be computed. For functional model.layers[i].output can be used.\n",
    "    :param data: dataset.\n",
    "    :param batch_size: batch_size of batches in which dataset should be divided.\n",
    "    :return: activations for a specific layer for all\n",
    "    \"\"\"\n",
    "\n",
    "    if len(data) % batch_size != 0:\n",
    "        data = data[: -(len(data) % batch_size)]\n",
    "\n",
    "    return Model(layer_in, layer_out).predict(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "batch_size=512\n",
    "epochs = 5\n",
    "act='relu'\n",
    "\n",
    "\n",
    "def create_ann():\n",
    "    inputs = tf.keras.Input(shape=(784,))\n",
    "    x = tf.keras.layers.Dense(500, activation=act)(inputs)\n",
    "    #x = tf.keras.layers.ReLU()(x)  # max_value=1\n",
    "    x = tf.keras.layers.Dense(100, activation=act)(x)\n",
    "    #x = tf.keras.layers.Activation(tf.nn.relu)(x)  # not implemented yet\n",
    "    x = tf.keras.layers.Dense(10, activation=act)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    ann = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    ann.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    ann.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.7013 - sparse_categorical_accuracy: 0.7837\n",
      "Epoch 2/5\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.1656 - sparse_categorical_accuracy: 0.9499\n",
      "Epoch 3/5\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.1091 - sparse_categorical_accuracy: 0.9670\n",
      "Epoch 4/5\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9773\n",
      "Epoch 5/5\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9831\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Import Data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "x_train = x_train.reshape((60000, 784))\n",
    "x_test = x_test.reshape((10000, 784))\n",
    "\n",
    "# Analog model\n",
    "ann = create_ann()\n",
    "\n",
    "# _, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# #weights = ann.get_weights()\n",
    "# weights = get_normalized_weights(ann, x_train, percentile=85)\n",
    "\n",
    "# ##################################################\n",
    "# # Preprocessing for RNN\n",
    "# x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "# x_test = np.expand_dims(x_test, axis=1)\n",
    "# #x_rnn = np.tile(x_train, (1, 1, 1))\n",
    "# #y_rnn = y_train  # np.tile(x_test, (1, timesteps, 1))\n",
    "\n",
    "# ##################################################\n",
    "# # Conversion to spiking model\n",
    "# snn = convert(ann, weights, x_test, y_test)\n",
    "# evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_original = ann.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 443,610\n",
      "Trainable params: 443,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-d7061c597c31>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  weights = np.array([weights_original[0], weights_original[2], weights_original[4]])\n",
      "<ipython-input-41-d7061c597c31>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  bias = np.array([weights_original[1], weights_original[3], weights_original[5]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 500 is out of bounds for axis 0 with size 500",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-41-d7061c597c31>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;31m# converted_weights_data_or_robust = weight_conversion_robust_and_data_based(weights, bias, model, ds_train)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m \u001B[0mconverted_weights_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mweight_conversion_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;31m### Applying weights to the architecture\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-8d21b06de7d0>\u001B[0m in \u001B[0;36mweight_conversion_model\u001B[0;34m(weights, bias)\u001B[0m\n\u001B[1;32m     20\u001B[0m                 \u001B[0minput_sum\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaximum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconverted_weights\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mconverted_bias\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mconverted_bias\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m                 \u001B[0minput_sum\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaximum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconverted_bias\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mo\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m             \u001B[0mmax_pos_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaximum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_pos_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_sum\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 500 is out of bounds for axis 0 with size 500"
     ]
    }
   ],
   "source": [
    "weights = np.array([weights_original[0], weights_original[2], weights_original[4]])\n",
    "bias = np.array([weights_original[1], weights_original[3], weights_original[5]])\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def flatten(image, label):\n",
    "    '''Transform image to the flattened version of itself'''\n",
    "    flattened = tf.reshape(image, [28*28])\n",
    "    flattened = tf.expand_dims(flattened, 0)\n",
    "    return tf.cast(flattened, tf.float32), label\n",
    "\n",
    "model = ann\n",
    "\n",
    "(ds_train, ds_test) = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)\n",
    "ds_train = ds_train.map(flatten, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "result = get_activations_layer(model.input, model.layers[1].output, ds_train)\n",
    "\n",
    "\n",
    "# converted_weights_data_or_robust = weight_conversion_robust_and_data_based(weights, bias, model, ds_train)\n",
    "\n",
    "converted_weights_model = weight_conversion_model(weights, bias)\n",
    "\n",
    "### Applying weights to the architecture\n",
    "\n",
    "# converted_weights = weight_conversion(weights)\n",
    "\n",
    "# expanded_converted_wt_0 = tf.expand_dims(converted_weights[0], 0)\n",
    "# expanded_converted_wt_1 = tf.expand_dims(converted_weights[1], 0)\n",
    "\n",
    "# model.layers[1].set_weights(expanded_converted_wt_0)\n",
    "# model.layers[2].set_weights(expanded_converted_wt_1)\n",
    "\n",
    "\n",
    "# model.evaluate(ds_test)\n",
    "\n",
    "# expanded_wt_0 = tf.expand_dims(weights[0], 0)\n",
    "# expanded_wt_1 = tf.expand_dims(weights[2], 0)\n",
    "\n",
    "# model.layers[1].set_weights(expanded_wt_0)\n",
    "# model.layers[2].set_weights(expanded_wt_1)\n",
    "\n",
    "# model.layers[1].get_weights()[1]\n",
    "\n",
    "# model.evaluate(ds_test)\n",
    "\n",
    "# random_wt_0 = tf.random.normal(weights[0].shape)\n",
    "# random_wt_0 = tf.expand_dims(random_wt_0, 0)\n",
    "\n",
    "# random_wt_1 = tf.random.normal(weights[2].shape)\n",
    "# random_wt_1 = tf.expand_dims(random_wt_1, 0)\n",
    "\n",
    "# model.layers[1].set_weights(random_wt_0)\n",
    "# model.layers[2].set_weights(random_wt_1)\n",
    "\n",
    "# model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([ 1.2715519e-05, -1.0592952e-05,  2.0176419e-06,  3.0930821e-06,\n",
       "       -5.3669709e-07,  2.4352505e-05, -2.1009200e-05,  1.2117722e-05,\n",
       "       -3.1248940e-06,  7.6323986e-06,  7.8802432e-06, -4.1792882e-06,\n",
       "        8.4965059e-06, -4.2733741e-06,  5.0357944e-06, -1.2094863e-05,\n",
       "        2.3230412e-05,  2.7503418e-06,  7.2594089e-06,  2.6647469e-05,\n",
       "        2.2722406e-05,  6.2220679e-06,  2.4737466e-05,  1.2531269e-05,\n",
       "       -1.3302352e-05,  1.7394526e-05,  1.2308599e-05,  1.3999711e-05,\n",
       "        3.6008103e-06,  2.2414417e-05,  8.9935929e-06,  4.0104916e-07,\n",
       "        2.7079677e-05,  8.0010077e-06,  7.9747588e-06,  9.2475702e-06,\n",
       "        1.6139924e-05,  1.0039097e-05,  8.7524704e-06,  1.3726043e-05,\n",
       "        8.7641383e-06, -6.0516055e-07,  2.0943609e-05, -7.2720131e-06,\n",
       "        6.4489186e-06,  8.1431508e-06,  2.1846565e-05,  2.9648354e-05,\n",
       "        1.5644033e-05,  2.7622764e-05,  1.3171528e-05,  3.7484332e-07,\n",
       "        6.3031948e-06, -8.2723373e-06,  1.6305408e-05,  1.6794171e-05,\n",
       "        2.6641932e-05,  9.7282600e-06,  1.6021313e-05,  1.2710798e-05,\n",
       "        2.0196223e-05, -3.1351210e-06, -3.6653735e-06,  3.0810101e-05,\n",
       "        2.7584347e-05,  2.2350379e-05,  3.0591251e-05,  2.8527234e-05,\n",
       "        1.6658181e-05,  6.3968346e-06,  1.1718271e-05,  3.4995471e-06,\n",
       "        1.5397245e-05,  1.6963735e-06,  2.4321241e-07, -3.7753784e-06,\n",
       "        1.5555210e-05,  2.3624536e-05,  3.1795058e-05, -1.2882322e-05,\n",
       "        1.8672798e-05, -3.2909888e-06,  8.4462845e-06,  2.0810596e-05,\n",
       "        2.4251633e-05,  8.6039317e-06,  2.3265287e-05,  2.3081249e-05,\n",
       "        2.2417436e-05,  3.8788576e-06,  2.4071298e-05, -9.7762249e-06,\n",
       "        1.9107450e-05, -4.2627252e-07,  1.3007560e-05,  2.5818068e-05,\n",
       "        1.8313100e-05,  6.3947850e-06, -3.4784425e-06,  3.8757544e-06],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_weights_data_or_robust[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
       "array([ 1.2715519e-05, -1.0592952e-05,  2.0176419e-06,  3.0930821e-06,\n",
       "       -5.3669709e-07,  2.4352505e-05, -2.1009200e-05,  1.2117722e-05,\n",
       "       -3.1248940e-06,  7.6323986e-06,  7.8802432e-06, -4.1792882e-06,\n",
       "        8.4965059e-06, -4.2733741e-06,  5.0357944e-06, -1.2094863e-05,\n",
       "        2.3230412e-05,  2.7503418e-06,  7.2594089e-06,  2.6647469e-05,\n",
       "        2.2722406e-05,  6.2220679e-06,  2.4737466e-05,  1.2531269e-05,\n",
       "       -1.3302352e-05,  1.7394526e-05,  1.2308599e-05,  1.3999711e-05,\n",
       "        3.6008103e-06,  2.2414417e-05,  8.9935929e-06,  4.0104916e-07,\n",
       "        2.7079677e-05,  8.0010077e-06,  7.9747588e-06,  9.2475702e-06,\n",
       "        1.6139924e-05,  1.0039097e-05,  8.7524704e-06,  1.3726043e-05,\n",
       "        8.7641383e-06, -6.0516055e-07,  2.0943609e-05, -7.2720131e-06,\n",
       "        6.4489186e-06,  8.1431508e-06,  2.1846565e-05,  2.9648354e-05,\n",
       "        1.5644033e-05,  2.7622764e-05,  1.3171528e-05,  3.7484332e-07,\n",
       "        6.3031948e-06, -8.2723373e-06,  1.6305408e-05,  1.6794171e-05,\n",
       "        2.6641932e-05,  9.7282600e-06,  1.6021313e-05,  1.2710798e-05,\n",
       "        2.0196223e-05, -3.1351210e-06, -3.6653735e-06,  3.0810101e-05,\n",
       "        2.7584347e-05,  2.2350379e-05,  3.0591251e-05,  2.8527234e-05,\n",
       "        1.6658181e-05,  6.3968346e-06,  1.1718271e-05,  3.4995471e-06,\n",
       "        1.5397245e-05,  1.6963735e-06,  2.4321241e-07, -3.7753784e-06,\n",
       "        1.5555210e-05,  2.3624536e-05,  3.1795058e-05, -1.2882322e-05,\n",
       "        1.8672798e-05, -3.2909888e-06,  8.4462845e-06,  2.0810596e-05,\n",
       "        2.4251633e-05,  8.6039317e-06,  2.3265287e-05,  2.3081249e-05,\n",
       "        2.2417436e-05,  3.8788576e-06,  2.4071298e-05, -9.7762249e-06,\n",
       "        1.9107450e-05, -4.2627252e-07,  1.3007560e-05,  2.5818068e-05,\n",
       "        1.8313100e-05,  6.3947850e-06, -3.4784425e-06,  3.8757544e-06],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}