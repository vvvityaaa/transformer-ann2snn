{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from spiking_models import DenseRNN, SpikingReLU, SpikingSigmoid, SpikingTanh, Accumulate\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Input, Model\n",
    "from operations_layers import SqueezeLayer, ExpandLayer, MatMulLayer, MatMulLayerTranspose, TransposeLayer, \\\n",
    "    ExtractPatchesLayer, PositionalEncodingLayer, Tokpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_weights(model, x_test, percentile=100):\n",
    "    x_test = x_test[::10]\n",
    "    max_activation = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.ReLU):\n",
    "            activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "            if np.percentile(activation, percentile) > max_activation:\n",
    "                max_activation = np.percentile(activation, percentile)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            if layer.activation.__name__ == 'relu':\n",
    "                activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "                if np.percentile(activation, percentile) > max_activation:\n",
    "                    max_activation = np.percentile(activation, percentile)\n",
    "\n",
    "    weights = model.get_weights()     \n",
    "    if max_activation == 0:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNo normalization\\n\" + \"-\"*32)\n",
    "    else:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNormalizing by\", max_activation, \"\\n\" + \"-\"*32)\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] /= (max_activation)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def evaluate_conversion(converted_model, original_model, x_test, y_test, testacc, timesteps=50):\n",
    "    for i in range(1, timesteps+1):\n",
    "        _, acc = converted_model.evaluate(x_test, y_test, batch_size=y_test.shape[0], verbose=0)\n",
    "        print(\n",
    "            \"Timesteps\", str(i) + \"/\" + str(timesteps) + \" -\",\n",
    "            \"acc spiking (orig): %.2f%% (%.2f%%)\" % (acc*100, testacc*100),\n",
    "            \"- conv loss: %+.2f%%\" % ((-(1 - acc/testacc)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_weight_normalization(model, x_test, ppercentile=1):\n",
    "    prev_factor = 1\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.ReLU) or (isinstance(layer, tf.keras.layers.Dense) and\n",
    "                                                       layer.activation.__name__ == 'relu'):\n",
    "\n",
    "            activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "            activation = tf.math.reduce_max(activation, axis=0)\n",
    "            activation = tf.sort(activation)\n",
    "            max_act = activation[int(ppercentile * (len(activation) - 1))]\n",
    "\n",
    "            weights, bias = layer.get_weights()\n",
    "            max_wt = max(0, tf.math.reduce_max(weights))\n",
    "            max_bias = tf.math.reduce_max(bias)\n",
    "\n",
    "            max_wt_bias = max(max_bias, max_wt)\n",
    "\n",
    "            scale_factor = max(max_act, max_wt_bias)\n",
    "            applied_factor = scale_factor / prev_factor\n",
    "\n",
    "            weights = weights / applied_factor\n",
    "            bias = bias / scale_factor\n",
    "\n",
    "            prev_factor = scale_factor\n",
    "            layer.set_weights([weights, bias])\n",
    "            print(f\"Scale factor for layer {layer}\")\n",
    "            print(f\"{applied_factor}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_conversion_model(weights, bias):\n",
    "    \"\"\"\n",
    "    Simple model-based conversion model proposed by Diehl et al.\n",
    "    :param weights: weights of the network.\n",
    "    :param bias: bias of the network.\n",
    "    :return: rescaled weights.\n",
    "    \"\"\"\n",
    "    # Get weights from trained network\n",
    "    converted_weights = weights\n",
    "    converted_bias = bias\n",
    "\n",
    "    # model based normalization\n",
    "    previous_factor = 1\n",
    "    for l in range(len(converted_weights)):\n",
    "        max_pos_input = 0\n",
    "        # Find maximum input for this layer\n",
    "        for o in range(converted_weights[l].shape[0]):\n",
    "            input_sum = 0\n",
    "            for i in range(converted_weights[l].shape[1]):\n",
    "                input_sum += tf.math.maximum(0, converted_weights[l][o, i])\n",
    "            if converted_bias is not None and converted_bias[l] is not None:\n",
    "                input_sum += tf.math.maximum(0, converted_bias[l][o])\n",
    "            max_pos_input = tf.math.maximum(max_pos_input, input_sum)\n",
    "\n",
    "        # get the maximum weight in the layer, in case all weights are negative, max_pos_input would be zero, so we\n",
    "        # use the max weight to rescale instead\n",
    "        max_wt = tf.math.reduce_max(converted_weights[l])\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            max_bias = tf.math.reduce_max(converted_bias[l])\n",
    "            max_wt = tf.math.maximum(max_wt, max_bias)\n",
    "        scale_factor = tf.math.maximum(max_wt, max_pos_input)\n",
    "        # Rescale all weights\n",
    "        applied_factor = scale_factor / previous_factor\n",
    "        converted_weights[l] = converted_weights[l] / applied_factor\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            converted_bias[l] = converted_bias[l] / scale_factor\n",
    "        previous_factor = scale_factor\n",
    "        print(f\"Scale factor for this layer is {previous_factor}\")\n",
    "\n",
    "    return converted_weights, converted_bias\n",
    "\n",
    "\n",
    "def weight_conversion_robust_and_data_based(weights, bias, model, data, normalization_method='robust',\n",
    "                                            ppercentile=0.99):\n",
    "\n",
    "    \"\"\"\n",
    "    Two methods proposed by Diehl et al and Rueckauer et al. Both methods are data-based, so they use weights and\n",
    "    activations to find the best scaling factor.\n",
    "    :param weights: weights of the network.\n",
    "    :param bias: bias of the network.\n",
    "    :param model: ann model.\n",
    "    :param data: dataset to determine activations.\n",
    "    :param normalization_method: type of normalization - robust (Rueckauer) or data (Diehl).\n",
    "    :param ppercentile: percentile of the activation, which is taken from maximal activation.\n",
    "    :return: rescaled weights.\n",
    "    \"\"\"\n",
    "    if normalization_method == 'data':\n",
    "        ppercentile = 1.0\n",
    "\n",
    "    # Get weights from trained network\n",
    "    converted_weights = weights\n",
    "    converted_bias = bias\n",
    "\n",
    "    # use training set to find max_act for each neuron\n",
    "\n",
    "    activations = []\n",
    "    for l in range(0, len(converted_weights)):\n",
    "        activation = get_activations_layer(model.input, model.layers[l].output, data)\n",
    "        activation_per_neuron = [np.max(activation[:, i]) for i in range(activation.shape[1])]\n",
    "        activations.append(activation_per_neuron)\n",
    "\n",
    "    previous_factor = 1\n",
    "    for l in range(len(converted_weights)):\n",
    "        # get the p-percentile of the activation\n",
    "        pos_inputs = activations[l]\n",
    "        pos_inputs.sort()\n",
    "        max_act = pos_inputs[int(ppercentile * (len(pos_inputs) - 1))]\n",
    "        # get the maximum weight in the layer\n",
    "        max_wt = tf.math.reduce_max(converted_weights[l])\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            max_bias = tf.math.reduce_max(converted_bias[l])\n",
    "            max_wt = tf.math.maximum(max_wt, max_bias)\n",
    "        scale_factor = tf.math.maximum(max_wt, max_act)\n",
    "\n",
    "        applied_factor = scale_factor / previous_factor\n",
    "        # rescale weights\n",
    "        converted_weights[l] = converted_weights[l] / applied_factor\n",
    "\n",
    "        # rescale bias\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            converted_bias[l] = converted_bias[l] / scale_factor\n",
    "        previous_factor = scale_factor\n",
    "        print(f\"Scale factor for this layer is {previous_factor}\")\n",
    "\n",
    "    return converted_weights, converted_bias\n",
    "\n",
    "\n",
    "def get_activations_layer(layer_in, layer_out, data, batch_size=32):\n",
    "\n",
    "    \"\"\"\n",
    "    Getting activation for specific layer of neural network.\n",
    "    :param layer_in: input layer of a model. For sequential models first layer, for functional model.layers[0].input can\n",
    "    be used.\n",
    "    :param layer_out: layer for which activations should be computed. For functional model.layers[i].output can be used.\n",
    "    :param data: dataset.\n",
    "    :param batch_size: batch_size of batches in which dataset should be divided.\n",
    "    :return: activations for a specific layer for all\n",
    "    \"\"\"\n",
    "\n",
    "    if len(data) % batch_size != 0:\n",
    "        data = data[: -(len(data) % batch_size)]\n",
    "\n",
    "    return Model(layer_in, layer_out).predict(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/viktor/.local/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/viktor/.local/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 [==============================] - 21s 105ms/step - loss: 0.6055 - accuracy: 0.6114 - val_loss: 0.3143 - val_accuracy: 0.8639\n",
      "Epoch 2/2\n",
      "196/196 [==============================] - 20s 103ms/step - loss: 0.2253 - accuracy: 0.9126 - val_loss: 0.3051 - val_accuracy: 0.8753\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tokpos (Tokpos)                 (None, 200, 32)      326400      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200, 32)      1056        tokpos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200, 32)      1056        tokpos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 50, 4)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 32, 50, 4)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer_1 (TransposeLay (None, 50, 32, 4)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer_2 (TransposeLay (None, 50, 32, 4)    0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200, 32)      1056        tokpos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_transpose (MatMul (None, 50, 32, 32)   0           transpose_layer_1[0][0]          \n",
      "                                                                 transpose_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 50, 4)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 50, 32, 32)   0           mat_mul_layer_transpose[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer (TransposeLayer (None, 50, 32, 4)    0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer (MatMulLayer)     (None, 50, 32, 4)    0           softmax[0][0]                    \n",
      "                                                                 transpose_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer_3 (TransposeLay (None, 32, 50, 4)    0           mat_mul_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 4, 50, 32)    0           transpose_layer_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4, 50, 32)    1056        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 4, 50, 32)    0           tokpos[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4, 50, 32)    0           dense_3[0][0]                    \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4, 50, 64)    2112        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4, 50, 32)    2080        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 4, 50, 32)    0           dense_5[0][0]                    \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6400)         0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           204832      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 32)           1056        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           2112        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            130         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 2)            0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 542,946\n",
      "Trainable params: 542,946\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "batch_size=128\n",
    "epochs=2\n",
    "dv = 25\n",
    "nv = -1\n",
    "vocab_size = 10000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "mlp_dim = 64\n",
    "l = 50\n",
    "num_heads = 4\n",
    "num_classes = 2\n",
    "\n",
    "def multi_head_attention(x):\n",
    "    # ================== Multi Head Self Attention ===============\n",
    "    v2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    q2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    k2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "\n",
    "    v = tf.keras.layers.Reshape([embed_dim, l, num_heads])(v2)\n",
    "    v = TransposeLayer()(v)\n",
    "    q = tf.keras.layers.Reshape([embed_dim, l, num_heads])(q2)\n",
    "    q = TransposeLayer()(q)\n",
    "    k = tf.keras.layers.Reshape([embed_dim, l, num_heads])(k2)\n",
    "    k = TransposeLayer()(k)\n",
    "\n",
    "    # =============== Scaled dot-product attention =================\n",
    "    # QK^T\n",
    "    att = MatMulLayerTranspose()([q, k])\n",
    "    # softmax(QK^T)\n",
    "    att = tf.keras.layers.Softmax(axis=-1)(att)\n",
    "    # softmax(QK^T)*V\n",
    "    out = MatMulLayer()([att, v])\n",
    "\n",
    "    att = TransposeLayer()(out)\n",
    "    out = tf.keras.layers.Reshape([-1, l, embed_dim])(att)\n",
    "    out = tf.keras.layers.Dense(embed_dim)(out)\n",
    "    # out = tf.keras.layers.Reshape([l, d_model, 1])(out)\n",
    "    x = tf.keras.layers.Reshape([-1, l, embed_dim])(x)\n",
    "    # ============== End of Multi Head Self Attention =============\n",
    "    # Concat Layer\n",
    "    add = tf.keras.layers.Add()([out, x])\n",
    "    # ================== End of Transformer =======================\n",
    "    return out, add\n",
    "\n",
    "def create_ann_approved_version():\n",
    "    inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "    x = Tokpos(maxlen, vocab_size, embed_dim)(inputs)\n",
    "    out = x\n",
    "    for i in range(1):\n",
    "        out, add = multi_head_attention(out)\n",
    "        out = tf.keras.layers.Dense(mlp_dim, activation=\"relu\")(add)\n",
    "        out = tf.keras.layers.Dense(embed_dim)(out)\n",
    "        out = tf.keras.layers.Add()([out, add])\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    x = tf.keras.layers.Dense(mlp_dim, activation=\"relu\")(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    \n",
    "    ann = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    ann.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    ann.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    return ann\n",
    "\n",
    "\n",
    "def convert_tailored_approved_version(weights, y_test):\n",
    "    inputs = tf.keras.layers.Input(shape=(1, maxlen,), batch_size=y_test.shape[0])\n",
    "    x = Tokpos(maxlen, vocab_size, embed_dim)(inputs)\n",
    "    out = x\n",
    "    for i in range(1):\n",
    "        out, add = multi_head_attention(out)\n",
    "        out = tf.keras.layers.Dense(mlp_dim)(add)\n",
    "        out = tf.keras.layers.Reshape([1, num_heads*l*mlp_dim])(out)\n",
    "        out = tf.keras.layers.RNN(SpikingReLU(num_heads*l*mlp_dim), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(out)\n",
    "        out = tf.keras.layers.Reshape([num_heads, l, mlp_dim])(out)\n",
    "        \n",
    "        out = tf.keras.layers.Dense(embed_dim)(out)\n",
    "        out = tf.keras.layers.Add()([out, add])\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = ExpandLayer()(x)\n",
    "    x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(embed_dim), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(x)\n",
    "    x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    x = tf.keras.layers.Dense(mlp_dim)(x)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(mlp_dim), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "    \n",
    "    x = tf.keras.layers.RNN(Accumulate(num_classes), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    \n",
    "    x = SqueezeLayer()(x)\n",
    "    \n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "    spiking.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    print(spiking.summary())\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Analog model\n",
    "ann = create_ann_approved_version()\n",
    "print(ann.summary())\n",
    "\n",
    "_, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# weights = ann.get_weights()\n",
    "# weights = get_normalized_weights(ann, x_train, percentile=85)\n",
    "\n",
    "model_normalized = robust_weight_normalization(ann, x_train)\n",
    "weights = model_normalized.get_weights()\n",
    "\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "# snn = convert(ann, weights, x_test, y_test)\n",
    "snn = convert_tailored_approved_version(weights, y_test)\n",
    "evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}