{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from snn_conversion.spiking_models import SpikingReLU, Accumulate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from snn_conversion.operations_layers import SqueezeLayer, ExpandLayer, ExtractPatchesLayer, PositionalEncodingLayer\n",
    "from snn_conversion.weight_normalization import robust_weight_normalization\n",
    "from snn_conversion.utils import evaluate_conversion, evaluate_conversion_and_save_data\n",
    "from snn_conversion.multi_head_self_attention import multi_head_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.5075 - accuracy: 0.8435 - val_loss: 0.1006 - val_accuracy: 0.9684\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0959 - accuracy: 0.9716 - val_loss: 0.0681 - val_accuracy: 0.9769\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "extract_patches_layer_1 (Extrac (None, None, 16)     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, None, 64)     1088        extract_patches_layer_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoding_layer_1 (Po (None, 50, 64)       3264        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 50, 64)       4160        positional_encoding_layer_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 50, 64)       4160        positional_encoding_layer_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 50, 4, 16)    0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 50, 4, 16)    0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer_5 (TransposeLay (None, 4, 50, 16)    0           reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer_6 (TransposeLay (None, 4, 50, 16)    0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 50, 64)       4160        positional_encoding_layer_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_transpose_1 (MatM (None, 4, 50, 50)    0           transpose_layer_5[0][0]          \n",
      "                                                                 transpose_layer_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 50, 4, 16)    0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax_2 (Softmax)             (None, 4, 50, 50)    0           mat_mul_layer_transpose_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer_4 (TransposeLay (None, 4, 50, 16)    0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_1 (MatMulLayer)   (None, 4, 50, 16)    0           softmax_2[0][0]                  \n",
      "                                                                 transpose_layer_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "transpose_layer_7 (TransposeLay (None, 50, 4, 16)    0           mat_mul_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 50, 64)    0           transpose_layer_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1, 50, 64)    4160        reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 50, 64)    0           positional_encoding_layer_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1, 50, 64)    0           dense_13[0][0]                   \n",
      "                                                                 reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 50, 128)   8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1, 50, 64)    8256        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 50, 64)    0           dense_15[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3200)         0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 64)           204864      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 10)           650         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax_3 (Softmax)             (None, 10)           0           dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 243,082\n",
      "Trainable params: 243,082\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Normalizing weights\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-98315e7676f7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[0;31m# weights = get_normalized_weights(ann, x_train, percentile=85)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Normalizing weights\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m \u001B[0mmodel_normalized\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrobust_weight_normalization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mann\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.99\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    155\u001B[0m \u001B[0mweights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_normalized\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-b328fd27499f>\u001B[0m in \u001B[0;36mrobust_weight_normalization\u001B[0;34m(model, x_test, ppercentile)\u001B[0m\n\u001B[1;32m     17\u001B[0m             \u001B[0mmax_wt_bias\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_bias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_wt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m             \u001B[0mscale_factor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_act\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_wt_bias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m             \u001B[0mapplied_factor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mscale_factor\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mprev_factor\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36m__bool__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    990\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    991\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m__bool__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 992\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    993\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    994\u001B[0m   \u001B[0m__nonzero__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m__bool__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def create_and_train_ann():\n",
    "\n",
    "    \"\"\"\n",
    "    Definition and training of artificial neural network with defined architecture in a keras functional API way.\n",
    "\n",
    "    :return: trained artificial neural network\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "    patches = ExtractPatchesLayer()(inputs)\n",
    "    x = tf.keras.layers.Dense(d_model)(patches)\n",
    "    x = PositionalEncodingLayer(d_model, num_patches)(x)\n",
    "\n",
    "    out = x\n",
    "    for _ in range(num_multi_head_attention_modules):\n",
    "        out = multi_head_self_attention(out, num_heads, projection_dim, d_model)\n",
    "        x = tf.keras.layers.Reshape([-1, d_model])(x)\n",
    "        add = tf.keras.layers.Add()([out, x])\n",
    "\n",
    "        # feedforward mlp\n",
    "        out = tf.keras.layers.Dense(mlp_dim, activation=\"relu\")(add)\n",
    "        out = tf.keras.layers.Dense(d_model)(out)\n",
    "\n",
    "        out = tf.keras.layers.Add()([out, add])\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = tf.keras.layers.Dense(mlp_dim, activation=\"relu\")(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "\n",
    "    ann = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    ann.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    ann.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    return ann\n",
    "\n",
    "\n",
    "def create_and_train_snn(weights, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Definition of spiking neural network. It copies ann network up to the dense layers with relu activation functions,\n",
    "    which are translated into rnn layers with SpikingReLU cells (neurons). This network is not trained, it's weights\n",
    "    are filled with normalized weights of artificial neural network.\n",
    "    :param weights: normalized weights from ann\n",
    "    :param y_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1), batch_size=y_test.shape[0])\n",
    "\n",
    "    patches = ExtractPatchesLayer()(inputs)\n",
    "    x = tf.keras.layers.Dense(d_model)(patches)\n",
    "    x = PositionalEncodingLayer(d_model, num_patches)(x)\n",
    "    out = x\n",
    "    for _ in range(num_multi_head_attention_modules):\n",
    "        out = multi_head_self_attention(out, num_heads, projection_dim, d_model)\n",
    "        x = tf.keras.layers.Reshape([-1, d_model])(x)\n",
    "        add = tf.keras.layers.Add()([out, x])\n",
    "\n",
    "        # feedforward mlp\n",
    "        out = tf.keras.layers.Dense(mlp_dim)(add)\n",
    "        out = tf.keras.layers.Reshape([1, l * mlp_dim])(out)\n",
    "        out = tf.keras.layers.RNN(SpikingReLU(l * mlp_dim), return_sequences=True, return_state=False,\n",
    "                                  stateful=True)(out)\n",
    "        out = tf.keras.layers.Reshape([-1, mlp_dim])(out)\n",
    "        out = tf.keras.layers.Dense(d_model)(out)\n",
    "\n",
    "        out = tf.keras.layers.Add()([out, add])\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = ExpandLayer()(x)\n",
    "    x = tf.keras.layers.Dense(mlp_dim)(x)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(mlp_dim), return_sequences=True, return_state=False,\n",
    "                            stateful=True)(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "\n",
    "    x = tf.keras.layers.RNN(Accumulate(num_classes), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "\n",
    "    x = SqueezeLayer()(x)\n",
    "\n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    print(\"-\" * 32 + \"\\n\")\n",
    "    spiking.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    print(spiking.summary())\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking\n",
    "\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "d_model = 64\n",
    "mlp_dim = 128\n",
    "l = 50\n",
    "num_heads = 4\n",
    "num_classes = 10\n",
    "channels = 1\n",
    "image_size = 28\n",
    "patch_size = 4\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "patch_dim = channels * patch_size ** 2\n",
    "projection_dim = d_model // num_heads\n",
    "num_multi_head_attention_modules = 4\n",
    "timesteps = 50\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize input so we can train ANN with it.\n",
    "# Will be converted back to integers for SNN layer.\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# One-hot encode target vectors.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Analog model\n",
    "ann = create_and_train_ann()\n",
    "print(ann.summary())\n",
    "\n",
    "_, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "model_normalized = robust_weight_normalization(ann, x_test, ppercentile=0.99)\n",
    "weights = model_normalized.get_weights()\n",
    "\n",
    "# Preprocessing for RNN\n",
    "# Add a channel dimension.\n",
    "axis = 1 if keras.backend.image_data_format() == 'channels_first' else -1\n",
    "x_train_expanded = np.expand_dims(x_train, axis)\n",
    "x_test_expanded = np.expand_dims(x_test, axis)\n",
    "\n",
    "# Conversion to spiking model\n",
    "snn = create_and_train_snn(weights, y_test)\n",
    "\n",
    "print(\"Simulating network\")\n",
    "evaluate_conversion(snn, x_test_expanded, y_test, testacc, y_test.shape[0], timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}