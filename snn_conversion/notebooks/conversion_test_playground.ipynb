{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from spiking_models import DenseRNN, SpikingReLU, SpikingSigmoid, SpikingTanh, Accumulate\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SqueezeLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(inputs, axis=1)\n",
    "    \n",
    "\n",
    "class ExpandLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ExpandLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=1)\n",
    "    \n",
    "\n",
    "class Tokpos(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Tokpos, self).__init__()\n",
    "        # TODO: remove fixed parametrization\n",
    "        self.maxlen = 200\n",
    "        self.vocab_size = 20000\n",
    "        self.embed_dim = 32\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, name=\"positional\")\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, name=\"token\")\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class Tokposangles(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Tokposangles, self).__init__()\n",
    "        # TODO: remove fixed parametrization\n",
    "        self.maxlen = 200\n",
    "        self.vocab_size = 20000\n",
    "        self.embed_dim = 32\n",
    "        # self.pos_emb = tf.keras.layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, name=\"positional\")\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, name=\"token\")\n",
    "        self.positions = self.positional_encoding(self.maxlen, self.embed_dim)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.token_emb(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        return x + self.positions[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class MatMulLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1])\n",
    "\n",
    "\n",
    "class MatMulLayerTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayerTranspose, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1], transpose_b=True)/np.sqrt(inputs[0].shape[-1])\n",
    "\n",
    "    \n",
    "class TransposeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TransposeLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(model, weights, x_test, y_test):\n",
    "    k = 0\n",
    "    print(\"Converted model:\\n\" + \"-\"*32)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            print(\"Input Layer\")\n",
    "            inputs = tf.keras.Input(shape=(1, model.layers[0].input_shape[0][1]), batch_size=y_test.shape[0])\n",
    "            x = inputs        \n",
    "#             x = ExpandLayer()(x)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            x = tf.keras.layers.Dense(layer.output_shape[1])(x)\n",
    "            # x = tf.keras.layers.RNN(DenseRNN(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            if layer.activation.__name__ == 'linear':\n",
    "                print(\"Dense Layer w/o activation\")\n",
    "                pass\n",
    "            elif layer.activation.__name__ == 'relu':\n",
    "                print(layer.output_shape[1])\n",
    "                print(\"Dense Layer with SpikingReLU\")\n",
    "                if k == 0:\n",
    "                    x = ExpandLayer()(x)\n",
    "                    k = 1\n",
    "                x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'sigmoid':\n",
    "                print(\"Dense Layer with SpikingSigmoid\")\n",
    "                x = tf.keras.layers.RNN(SpikingSigmoid(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'tanh':\n",
    "                print(\"Dense Layer with SpikingTanh\")\n",
    "                x = tf.keras.layers.RNN(SpikingTanh(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            else:\n",
    "                print('[Info] Activation type', layer.activation.__name__, 'not implemented')\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            print(\"SpikingReLU Layer\")\n",
    "            print(layer.output_shape[1])\n",
    "            x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "        elif isinstance(layer, tf.keras.layers.Softmax):\n",
    "            print(\"Accumulate + Softmax Layer\")\n",
    "            print(layer.output_shape[1])\n",
    "            x = tf.keras.layers.RNN(Accumulate(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            x = tf.keras.layers.Softmax()(x)\n",
    "        elif isinstance(layer, tf.keras.layers.Embedding):\n",
    "            print(\"Embedding Layer\")\n",
    "            x = tf.keras.layers.Embedding(input_dim=20000, output_dim=32)(x)\n",
    "        elif isinstance(layer, Tokpos):\n",
    "            print(\"Tokpos Layer\")\n",
    "            x = Tokpos()(x)\n",
    "        elif isinstance(layer, Tokposangles):\n",
    "            print(\"Tokposangles Layer\")\n",
    "            x = Tokposangles()(x)\n",
    "        elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "            print(\"Flatten Layer\")\n",
    "            x = tf.keras.layers.Flatten()(x)\n",
    "        elif isinstance(layer, ExpandLayer):\n",
    "            print(\"Expand Layer\")\n",
    "            x = ExpandLayer()(x)\n",
    "        elif isinstance(layer, SqueezeLayer):\n",
    "            print(\"Squeeze Layer\")\n",
    "            x = SqueezeLayer()(x)\n",
    "        elif isinstance(layer, MatMulLayer):\n",
    "            print(\"MatMulLayer Layer\")\n",
    "            x = MatMulLayer()(x)\n",
    "        elif isinstance(layer, MatMulLayerTranspose):\n",
    "            print(\"MatMulLayerTranspose Layer\")\n",
    "            x = MatMulLayerTranspose()(x)\n",
    "        else:\n",
    "            print(\"[Info] Layer type \", layer, \"not implemented\")\n",
    "#     x = SqueezeLayer()(x)\n",
    "    x = SqueezeLayer()(x)\n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "\n",
    "#     spiking.compile(\n",
    "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "#         optimizer=\"adam\")\n",
    "    \n",
    "    spiking.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    print(spiking.summary())\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_weights(model, x_test, percentile=100):\n",
    "#     x_test = x_test[::25]\n",
    "    max_activation = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.ReLU):\n",
    "            activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "            if np.percentile(activation, percentile) > max_activation:\n",
    "                max_activation = np.percentile(activation, percentile)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            if layer.activation.__name__ == 'relu':\n",
    "                activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "                if np.percentile(activation, percentile) > max_activation:\n",
    "                    max_activation = np.percentile(activation, percentile)\n",
    "\n",
    "    weights = model.get_weights()     \n",
    "    if max_activation == 0:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNo normalization\\n\" + \"-\"*32)\n",
    "    else:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNormalizing by\", max_activation, \"\\n\" + \"-\"*32)\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] /= (max_activation)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_conversion(converted_model, original_model, x_test, y_test, testacc, timesteps=50):\n",
    "    for i in range(1, timesteps+1):\n",
    "        _, acc = converted_model.evaluate(x_test, y_test, batch_size=y_test.shape[0], verbose=0)\n",
    "        print(\n",
    "            \"Timesteps\", str(i) + \"/\" + str(timesteps) + \" -\",\n",
    "            \"acc spiking (orig): %.2f%% (%.2f%%)\" % (acc*100, testacc*100),\n",
    "            \"- conv loss: %+.2f%%\" % ((-(1 - acc/testacc)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "epochs = 2\n",
    "act='relu'\n",
    "\n",
    "\n",
    "def create_ann():\n",
    "    inputs = tf.keras.Input(shape=(784,))\n",
    "    x = tf.keras.layers.Dense(500, activation=act)(inputs)\n",
    "    #x = tf.keras.layers.ReLU()(x)  # max_value=1\n",
    "    x = tf.keras.layers.Dense(100, activation=act)(x)\n",
    "    #x = tf.keras.layers.Activation(tf.nn.relu)(x)  # not implemented yet\n",
    "    x = tf.keras.layers.Dense(10)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    ann = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    ann.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    ann.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    return ann\n",
    "\n",
    "def create_ann_with_embedding():\n",
    "    dv = 25\n",
    "    nv = -1\n",
    "    vocab_size = 20000  # Only consider the top 20k words\n",
    "    maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "#     x = Tokposangles()(inputs)\n",
    "#     x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=32)(inputs)\n",
    "    x = Tokpos()(inputs)\n",
    "    \n",
    "    # -------------- TRANSFORMER BLOCK -----------------\n",
    "    v2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    q2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    k2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "\n",
    "    v = tf.keras.layers.Reshape([embed_dim, nv, dv])(v2)\n",
    "    q = tf.keras.layers.Reshape([embed_dim, nv, dv])(q2)\n",
    "    k = tf.keras.layers.Reshape([embed_dim, nv, dv])(k2)\n",
    "\n",
    "    # softmax(q*k^T/sqrt(dv))\n",
    "    att = MatMulLayerTranspose()([q, k])\n",
    "    # att = ScaleLayer()([att, np.sqrt(dv)])\n",
    "    # TODO: observer if axis=-1 is not necessary\n",
    "    att = tf.keras.layers.Softmax()(att)\n",
    "    # softmax(q*k^T/sqrt(dv))*v\n",
    "    out = MatMulLayer()([att, v])\n",
    "\n",
    "    out = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(out)\n",
    "    x = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(x)\n",
    "    add = tf.keras.layers.Add()([out, x])\n",
    "    # add = tf.add(out, x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(add)\n",
    "    x = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#     x = tf.keras.layers.AveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "    # --------------------------------------------------\n",
    "    outputs = tf.keras.layers.Dense(2)(x)\n",
    "    outputs = tf.keras.layers.Softmax()(outputs)\n",
    "    ann = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    ann.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    \n",
    "#     ann.compile(\n",
    "#         optimizer=tf.keras.optimizers.RMSprop(),\n",
    "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    ann.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer average_pooling1d_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1acd9806580a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Analog model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ann_with_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-99297aa8655a>\u001b[0m in \u001b[0;36mcreate_ann_with_embedding\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# --------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2682\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2684\u001b[0;31m       input_spec.assert_input_compatibility(\n\u001b[0m\u001b[1;32m   2685\u001b[0m           self.input_spec, inputs, self.name)\n\u001b[1;32m   2686\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[1;32m    220\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer average_pooling1d_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 32)"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1238)\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_test), \"Validation sequences\")\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "# x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "# x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Analog model\n",
    "ann = create_ann_with_embedding()\n",
    "print(ann.summary())\n",
    "\n",
    "_, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# weights = ann.get_weights()\n",
    "weights = get_normalized_weights(ann, x_train, percentile=99.9)\n",
    "\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "# y_train = np.expand_dims(y_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "# y_test = np.expand_dims(y_test, axis=1)\n",
    "#x_rnn = np.tile(x_train, (1, 1, 1))\n",
    "#y_rnn = y_train  # np.tile(x_test, (1, timesteps, 1))\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "snn = convert(ann, weights, x_test, y_test)\n",
    "evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tailored(weights, y_test):\n",
    "    dv = 25\n",
    "    nv = -1\n",
    "    vocab_size = 20000  # Only consider the top 20k words\n",
    "    maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(1, maxlen,), batch_size=y_test.shape[0])\n",
    "    x = Tokpos()(inputs)\n",
    "    \n",
    "    # -------------- TRANSFORMER BLOCK -----------------\n",
    "\n",
    "    v2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    q2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    k2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "\n",
    "    v = tf.keras.layers.Reshape([embed_dim, nv, dv])(v2)\n",
    "    q = tf.keras.layers.Reshape([embed_dim, nv, dv])(q2)\n",
    "    k = tf.keras.layers.Reshape([embed_dim, nv, dv])(k2)\n",
    "\n",
    "    # softmax(q*k^T/sqrt(dv))\n",
    "    att = MatMulLayerTranspose()([q, k])\n",
    "    # att = ScaleLayer()([att, np.sqrt(dv)])\n",
    "    # TODO: observer if axis=-1 is not necessary\n",
    "#     att = tf.keras.layers.Reshape([1, embed_dim*nv*dv])(att)\n",
    "#     print(att.shape)\n",
    "#     att = tf.keras.layers.RNN(Accumulate(embed_dim*dv), return_sequences=True, return_state=False, stateful=True)(att)\n",
    "#     att = tf.keras.layers.Reshape([embed_dim, nv, dv])(att)\n",
    "    att = tf.keras.layers.Softmax()(att)\n",
    "    \n",
    "    # softmax(q*k^T/sqrt(dv))*v\n",
    "    out = MatMulLayer()([att, v])\n",
    "\n",
    "    out = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(out)\n",
    "    x = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(x)\n",
    "    add = tf.keras.layers.Add()([out, x])\n",
    "    # add = tf.add(out, x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(add)\n",
    "    x = ExpandLayer()(x)\n",
    "    x = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(embed_dim), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(x)\n",
    "    x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    x = tf.keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(50), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(2)(x)\n",
    "    x = tf.keras.layers.RNN(Accumulate(2), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    x = SqueezeLayer()(x)\n",
    "    \n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "    spiking.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    print(spiking.summary())\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n",
      "Epoch 1/2\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5954 - accuracy: 0.6101 - val_loss: 0.3001 - val_accuracy: 0.8723\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1775 - accuracy: 0.9323 - val_loss: 0.3069 - val_accuracy: 0.8711\n",
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tokpos_36 (Tokpos)              (None, 200, 32)      646400      input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_214 (Dense)               (None, 200, 32)      1056        tokpos_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_215 (Dense)               (None, 200, 32)      1056        tokpos_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_177 (Reshape)           (None, 32, 8, 25)    0           dense_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_178 (Reshape)           (None, 32, 8, 25)    0           dense_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_transpose_36 (Mat (None, 32, 8, 8)     0           reshape_177[0][0]                \n",
      "                                                                 reshape_178[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, 200, 32)      1056        tokpos_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_55 (Softmax)            (None, 32, 8, 8)     0           mat_mul_layer_transpose_36[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_176 (Reshape)           (None, 32, 8, 25)    0           dense_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_32 (MatMulLayer)  (None, 32, 8, 25)    0           softmax_55[0][0]                 \n",
      "                                                                 reshape_176[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_179 (Reshape)           (None, 32, 200, 1)   0           mat_mul_layer_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_180 (Reshape)           (None, 32, 200, 1)   0           tokpos_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 32, 200, 1)   0           reshape_179[0][0]                \n",
      "                                                                 reshape_180[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 6400)         0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_216 (Dense)               (None, 32)           204832      flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_217 (Dense)               (None, 32)           1056        dense_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_218 (Dense)               (None, 50)           1650        dense_217[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_219 (Dense)               (None, 2)            102         dense_218[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_56 (Softmax)            (None, 2)            0           dense_219[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 857,208\n",
      "Trainable params: 857,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "--------------------------------\n",
      "Normalizing by 2.0405467867851255 \n",
      "--------------------------------\n",
      "--------------------------------\n",
      "\n",
      "Model: \"model_43\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_38 (InputLayer)           [(25000, 1, 200)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tokpos_37 (Tokpos)              (25000, 1, 200, 32)  646400      input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_221 (Dense)               (25000, 1, 200, 32)  1056        tokpos_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_222 (Dense)               (25000, 1, 200, 32)  1056        tokpos_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_182 (Reshape)           (25000, 32, 8, 25)   0           dense_221[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_183 (Reshape)           (25000, 32, 8, 25)   0           dense_222[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_transpose_37 (Mat (25000, 32, 8, 8)    0           reshape_182[0][0]                \n",
      "                                                                 reshape_183[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_220 (Dense)               (25000, 1, 200, 32)  1056        tokpos_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_57 (Softmax)            (25000, 32, 8, 8)    0           mat_mul_layer_transpose_37[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_181 (Reshape)           (25000, 32, 8, 25)   0           dense_220[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_33 (MatMulLayer)  (25000, 32, 8, 25)   0           softmax_57[0][0]                 \n",
      "                                                                 reshape_181[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_184 (Reshape)           (25000, 32, 200, 1)  0           mat_mul_layer_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_185 (Reshape)           (25000, 32, 200, 1)  0           tokpos_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (25000, 32, 200, 1)  0           reshape_184[0][0]                \n",
      "                                                                 reshape_185[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (25000, 6400)        0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "expand_layer_8 (ExpandLayer)    (25000, 1, 6400)     0           flatten_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_223 (Dense)               (25000, 1, 32)       204832      expand_layer_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "rnn_25 (RNN)                    (25000, 1, 32)       0           dense_223[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_224 (Dense)               (25000, 1, 32)       1056        rnn_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_225 (Dense)               (25000, 1, 50)       1650        dense_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "rnn_26 (RNN)                    (25000, 1, 50)       0           dense_225[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_226 (Dense)               (25000, 1, 2)        102         rnn_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "rnn_27 (RNN)                    (25000, 1, 2)        0           dense_226[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_58 (Softmax)            (25000, 1, 2)        0           rnn_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "squeeze_layer_5 (SqueezeLayer)  (25000, 2)           0           softmax_58[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 857,208\n",
      "Trainable params: 857,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps 1/50 - acc spiking (orig): 50.00% (87.11%) - conv loss: -42.60%\n",
      "Timesteps 2/50 - acc spiking (orig): 50.00% (87.11%) - conv loss: -42.60%\n",
      "Timesteps 3/50 - acc spiking (orig): 50.00% (87.11%) - conv loss: -42.60%\n",
      "Timesteps 4/50 - acc spiking (orig): 50.00% (87.11%) - conv loss: -42.60%\n",
      "Timesteps 5/50 - acc spiking (orig): 50.00% (87.11%) - conv loss: -42.60%\n",
      "Timesteps 6/50 - acc spiking (orig): 50.07% (87.11%) - conv loss: -42.52%\n",
      "Timesteps 7/50 - acc spiking (orig): 52.11% (87.11%) - conv loss: -40.18%\n",
      "Timesteps 8/50 - acc spiking (orig): 56.36% (87.11%) - conv loss: -35.30%\n",
      "Timesteps 9/50 - acc spiking (orig): 62.22% (87.11%) - conv loss: -28.57%\n",
      "Timesteps 10/50 - acc spiking (orig): 69.04% (87.11%) - conv loss: -20.74%\n",
      "Timesteps 11/50 - acc spiking (orig): 75.08% (87.11%) - conv loss: -13.81%\n",
      "Timesteps 12/50 - acc spiking (orig): 79.60% (87.11%) - conv loss: -8.62%\n",
      "Timesteps 13/50 - acc spiking (orig): 82.99% (87.11%) - conv loss: -4.73%\n",
      "Timesteps 14/50 - acc spiking (orig): 85.03% (87.11%) - conv loss: -2.39%\n",
      "Timesteps 15/50 - acc spiking (orig): 86.24% (87.11%) - conv loss: -1.01%\n",
      "Timesteps 16/50 - acc spiking (orig): 86.34% (87.11%) - conv loss: -0.89%\n",
      "Timesteps 17/50 - acc spiking (orig): 86.17% (87.11%) - conv loss: -1.08%\n",
      "Timesteps 18/50 - acc spiking (orig): 85.88% (87.11%) - conv loss: -1.41%\n",
      "Timesteps 19/50 - acc spiking (orig): 85.43% (87.11%) - conv loss: -1.93%\n",
      "Timesteps 20/50 - acc spiking (orig): 84.79% (87.11%) - conv loss: -2.67%\n",
      "Timesteps 21/50 - acc spiking (orig): 84.35% (87.11%) - conv loss: -3.17%\n",
      "Timesteps 22/50 - acc spiking (orig): 84.23% (87.11%) - conv loss: -3.31%\n",
      "Timesteps 23/50 - acc spiking (orig): 84.75% (87.11%) - conv loss: -2.71%\n",
      "Timesteps 24/50 - acc spiking (orig): 85.28% (87.11%) - conv loss: -2.11%\n",
      "Timesteps 25/50 - acc spiking (orig): 85.72% (87.11%) - conv loss: -1.59%\n",
      "Timesteps 26/50 - acc spiking (orig): 86.11% (87.11%) - conv loss: -1.15%\n",
      "Timesteps 27/50 - acc spiking (orig): 86.35% (87.11%) - conv loss: -0.88%\n",
      "Timesteps 28/50 - acc spiking (orig): 86.42% (87.11%) - conv loss: -0.79%\n",
      "Timesteps 29/50 - acc spiking (orig): 86.55% (87.11%) - conv loss: -0.64%\n",
      "Timesteps 30/50 - acc spiking (orig): 86.32% (87.11%) - conv loss: -0.91%\n",
      "Timesteps 31/50 - acc spiking (orig): 86.22% (87.11%) - conv loss: -1.03%\n",
      "Timesteps 32/50 - acc spiking (orig): 86.03% (87.11%) - conv loss: -1.24%\n",
      "Timesteps 33/50 - acc spiking (orig): 85.85% (87.11%) - conv loss: -1.45%\n",
      "Timesteps 34/50 - acc spiking (orig): 85.83% (87.11%) - conv loss: -1.47%\n",
      "Timesteps 35/50 - acc spiking (orig): 85.68% (87.11%) - conv loss: -1.64%\n",
      "Timesteps 36/50 - acc spiking (orig): 85.88% (87.11%) - conv loss: -1.41%\n",
      "Timesteps 37/50 - acc spiking (orig): 86.09% (87.11%) - conv loss: -1.18%\n",
      "Timesteps 38/50 - acc spiking (orig): 86.40% (87.11%) - conv loss: -0.82%\n",
      "Timesteps 39/50 - acc spiking (orig): 86.55% (87.11%) - conv loss: -0.65%\n",
      "Timesteps 40/50 - acc spiking (orig): 86.53% (87.11%) - conv loss: -0.67%\n",
      "Timesteps 41/50 - acc spiking (orig): 86.57% (87.11%) - conv loss: -0.62%\n",
      "Timesteps 42/50 - acc spiking (orig): 86.73% (87.11%) - conv loss: -0.44%\n",
      "Timesteps 43/50 - acc spiking (orig): 86.68% (87.11%) - conv loss: -0.49%\n",
      "Timesteps 44/50 - acc spiking (orig): 86.66% (87.11%) - conv loss: -0.52%\n",
      "Timesteps 45/50 - acc spiking (orig): 86.66% (87.11%) - conv loss: -0.52%\n",
      "Timesteps 46/50 - acc spiking (orig): 86.42% (87.11%) - conv loss: -0.79%\n",
      "Timesteps 47/50 - acc spiking (orig): 86.34% (87.11%) - conv loss: -0.88%\n",
      "Timesteps 48/50 - acc spiking (orig): 86.38% (87.11%) - conv loss: -0.84%\n",
      "Timesteps 49/50 - acc spiking (orig): 86.33% (87.11%) - conv loss: -0.90%\n",
      "Timesteps 50/50 - acc spiking (orig): 86.35% (87.11%) - conv loss: -0.88%\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1238)\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_test), \"Validation sequences\")\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "# x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "# x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Analog model\n",
    "ann = create_ann_with_embedding()\n",
    "print(ann.summary())\n",
    "\n",
    "_, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# weights = ann.get_weights()\n",
    "weights = get_normalized_weights(ann, x_train, percentile=95)\n",
    "\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "# y_train = np.expand_dims(y_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "# y_test = np.expand_dims(y_test, axis=1)\n",
    "#x_rnn = np.tile(x_train, (1, 1, 1))\n",
    "#y_rnn = y_train  # np.tile(x_test, (1, timesteps, 1))\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "# snn = convert(ann, weights, x_test, y_test)\n",
    "snn = convert_tailored(weights, y_test)\n",
    "evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion with approved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(x):\n",
    "    # ================== Multi Head Self Attention ===============\n",
    "    v2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    q2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    k2 = tf.keras.layers.Dense(embed_dim)(x)\n",
    "\n",
    "    v = tf.keras.layers.Reshape([embed_dim, l, num_heads])(v2)\n",
    "#     v = TransposeLayer()(v)\n",
    "    q = tf.keras.layers.Reshape([embed_dim, l, num_heads])(q2)\n",
    "#     q = TransposeLayer()(q)\n",
    "    k = tf.keras.layers.Reshape([embed_dim, l, num_heads])(k2)\n",
    "#     k = TransposeLayer()(k)\n",
    "\n",
    "    # =============== Scaled dot-product attention =================\n",
    "    # QK^T\n",
    "    att = MatMulLayerTranspose()([q, k])\n",
    "    # softmax(QK^T)\n",
    "    att = tf.keras.layers.Softmax(axis=-1)(att)\n",
    "    # softmax(QK^T)*V\n",
    "    out = MatMulLayer()([att, v])\n",
    "\n",
    "    att = TransposeLayer()(out)\n",
    "    out = tf.keras.layers.Reshape([-1, l, embed_dim])(att)\n",
    "    out = tf.keras.layers.Dense(embed_dim)(out)\n",
    "    # out = tf.keras.layers.Reshape([l, d_model, 1])(out)\n",
    "    x = tf.keras.layers.Reshape([-1, l, embed_dim])(x)\n",
    "    # ============== End of Multi Head Self Attention =============\n",
    "    # Concat Layer\n",
    "    add = tf.keras.layers.Add()([out, x])\n",
    "    \n",
    "    out = tf.keras.layers.Dense(mlp_dim)(add)\n",
    "#     out = tf.keras.layers.Dense(mlp_dim, activation=\"relu\")(add)\n",
    "    out = tf.keras.layers.Dense(embed_dim)(out)\n",
    "    out = tf.keras.layers.Add()([out, add])\n",
    "    # ================== End of Transformer =======================\n",
    "    return out\n",
    "\n",
    "def create_ann_approved_version():\n",
    "    dv = 25\n",
    "    nv = -1\n",
    "    vocab_size = 20000  # Only consider the top 20k words\n",
    "    maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "    embed_dim = 32  # Embedding size for each token\n",
    "    mlp_dim = 128\n",
    "    l = 50\n",
    "    num_heads = 4\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "    x = Tokpos()(inputs)\n",
    "    out = x\n",
    "    for i in range(1):\n",
    "        out = multi_head_attention(out)\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = tf.keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(2)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    \n",
    "    ann = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    ann.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    ann.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    return ann\n",
    "\n",
    "\n",
    "def convert_tailored_approved_version(weights, y_test):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=(maxlen,), batch_size=y_test.shape[0])\n",
    "    x = Tokpos()(inputs)\n",
    "    out = x\n",
    "    for i in range(1):\n",
    "        out = multi_head_attention(out)\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = ExpandLayer()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(50), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.RNN(Accumulate(2), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    \n",
    "    x = SqueezeLayer()(x)\n",
    "    \n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "    spiking.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    print(spiking.summary())\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking\n",
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(1238)\n",
    "dv = 25\n",
    "nv = -1\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "mlp_dim = 128\n",
    "l = 50\n",
    "num_heads = 4\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Analog model\n",
    "ann = create_ann_approved_version()\n",
    "print(ann.summary())\n",
    "\n",
    "_, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# weights = ann.get_weights()\n",
    "weights = get_normalized_weights(ann, x_train, percentile=95)\n",
    "\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "# snn = convert(ann, weights, x_test, y_test)\n",
    "snn = convert_tailored_approved_version(weights, y_test)\n",
    "evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run conversion original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Import Data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "x_train = x_train.reshape((60000, 784))\n",
    "x_test = x_test.reshape((10000, 784))\n",
    "\n",
    "# Analog model\n",
    "ann = create_ann()\n",
    "ann.summary()\n",
    "_, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# weights = ann.get_weights()\n",
    "weights = get_normalized_weights(ann, x_train, percentile=99.9)\n",
    "\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "#x_rnn = np.tile(x_train, (1, 1, 1))\n",
    "#y_rnn = y_train  # np.tile(x_test, (1, timesteps, 1))\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "snn = convert(ann, weights, x_test, y_test)\n",
    "evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST ViT Adapted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    \n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dim])\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaleLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        scale = inputs[1]\n",
    "        return inputs[0] / scale\n",
    "\n",
    "    \n",
    "class MatMulLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1])\n",
    "\n",
    "    \n",
    "class MatMulLayerTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayerTranspose, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1], transpose_b=True)\n",
    "\n",
    "    \n",
    "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PositionalEncodingLayer, self).__init__()\n",
    "        self.pos_emb = self.add_weight(\"pos_emb\", shape=(1, num_patches + 1, d_model))\n",
    "        self.class_emb = self.add_weight(\"class_emb\", shape=(1, 1, d_model))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        class_emb = tf.broadcast_to(self.class_emb, [batch_size, 1, d_model])\n",
    "        x = tf.concat([class_emb, inputs], axis=1)\n",
    "        return x + self.pos_emb\n",
    "\n",
    "    \n",
    "class ExtractPatchesLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ExtractPatchesLayer, self).__init__()\n",
    "        self.patch_size = 4\n",
    "        self.patch_dim = 16\n",
    "        \n",
    "    def extract_patches(self, images, patch_size, patch_dim):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, patch_size, patch_size, 1],\n",
    "            strides=[1, patch_size, patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dim])\n",
    "        return patches\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.extract_patches(inputs, self.patch_size, self.patch_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 6s 6ms/step - loss: 0.6276 - accuracy: 0.8023 - val_loss: 0.1368 - val_accuracy: 0.9575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd0a0706910>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fully Functional Modular version with Multiply layers instead of einsums\n",
    "\"\"\"\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize input so we can train ANN with it.\n",
    "# Will be converted back to integers for SNN layer.\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Add a channel dimension.\n",
    "axis = 1 if keras.backend.image_data_format() == 'channels_first' else -1\n",
    "x_train = np.expand_dims(x_train, axis)\n",
    "x_test = np.expand_dims(x_test, axis)\n",
    "\n",
    "# One-hot encode target vectors.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "patch_size = 4\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "channels = 1\n",
    "patch_dim = channels * patch_size ** 2\n",
    "batch_size = 64\n",
    "embed_dim = d_model = 64\n",
    "\n",
    "l = 50\n",
    "d = 192\n",
    "dv = 24\n",
    "dout = 32\n",
    "nv = 8\n",
    "\n",
    "inp = Input(shape=(28, 28, 1))\n",
    "patches = ExtractPatchesLayer()(inp)\n",
    "x = tf.keras.layers.Dense(d_model, activation=\"relu\")(patches)\n",
    "x = PositionalEncodingLayer()(x)\n",
    "\n",
    "# Attention Module\n",
    "v2 = tf.keras.layers.Dense(nv*nv, activation=\"relu\")(x)\n",
    "q2 = tf.keras.layers.Dense(nv*nv, activation=\"relu\")(x)\n",
    "k2 = tf.keras.layers.Dense(nv*nv, activation=\"relu\")(x)\n",
    "\n",
    "v = tf.keras.layers.Reshape([l, nv, nv])(v2)\n",
    "q = tf.keras.layers.Reshape([l, nv, nv])(q2)\n",
    "k = tf.keras.layers.Reshape([l, nv, nv])(k2)\n",
    "\n",
    "# =============== Scaled dot-product attention =================\n",
    "# QK^T\n",
    "att = MatMulLayerTranspose()([q, k])\n",
    "# softmax(QK^T)\n",
    "att = tf.keras.layers.Softmax(axis=-1)(att)\n",
    "# softmax(QK^T)*V\n",
    "out = MatMulLayer()([att, v])\n",
    "\n",
    "out = tf.keras.layers.Reshape([l, d_model, 1])(out)\n",
    "x = tf.keras.layers.Reshape([l, d_model, 1])(x)\n",
    "\n",
    "# Concat Layer\n",
    "add = tf.keras.layers.Add()([out, x])\n",
    "out = tf.keras.layers.Flatten()(add)\n",
    "\n",
    "out = tf.keras.layers.Dense(32, activation=\"relu\")(out)\n",
    "out = tf.keras.layers.Dense(num_classes, activation='softmax')(out)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "num_heads = 4\n",
    "mlp_dim = 128\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "def convert(model, weights, x_test, y_test):\n",
    "    print(\"Converted model:\\n\" + \"-\"*32)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            print(\"Input Layer\")\n",
    "            inputs = tf.keras.Input(shape=(1, model.layers[0].input_shape[0][1]), batch_size=y_test.shape[0])\n",
    "            x = inputs        \n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            x = tf.keras.layers.Dense(layer.output_shape[1])(x)\n",
    "            # x = tf.keras.layers.RNN(DenseRNN(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            if layer.activation.__name__ == 'linear':\n",
    "                print(\"Dense Layer w/o activation\")\n",
    "                pass\n",
    "            elif layer.activation.__name__ == 'relu':\n",
    "                print(\"Dense Layer with SpikingReLU\")\n",
    "                x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'sigmoid':\n",
    "                print(\"Dense Layer with SpikingSigmoid\")\n",
    "                x = tf.keras.layers.RNN(SpikingSigmoid(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'tanh':\n",
    "                print(\"Dense Layer with SpikingTanh\")\n",
    "                x = tf.keras.layers.RNN(SpikingTanh(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            else:\n",
    "                print('[Info] Activation type', layer.activation.__name__, 'not implemented')\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            print(\"SpikingReLU Layer\")\n",
    "            x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "        elif isinstance(layer, tf.keras.layers.Softmax):\n",
    "            print(\"Accumulate + Softmax Layer\")\n",
    "            print(layer.output_shape[1])\n",
    "            x = tf.keras.layers.RNN(Accumulate(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            x = tf.keras.layers.Softmax()(x)\n",
    "#         elif isinstance(layer, TransformerBlock):\n",
    "#             x  = SpikingTransformerBlock(d_model, num_heads, mlp_dim, dropout)\n",
    "        else:\n",
    "            print(\"[Info] Layer type \", layer, \"not implemented, so we just use it's non-spiking version\")\n",
    "            try:\n",
    "                x = layer(x)\n",
    "            except:\n",
    "                import pdb;pdb.set_trace()\n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "\n",
    "    spiking.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],)\n",
    "\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converted_model(weights):\n",
    "    num_classes = 10\n",
    "    image_size = 28\n",
    "    patch_size = 4\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    channels = 1\n",
    "    patch_dim = channels * patch_size ** 2\n",
    "    batch_size = 64\n",
    "    embed_dim = d_model = 64\n",
    "\n",
    "    l = 50\n",
    "    d = 192\n",
    "    dv = 24\n",
    "    dout = 32\n",
    "    nv = 8\n",
    "\n",
    "    inp = Input(shape=(28, 28, 1), batch_size=10000)\n",
    "    patches = ExtractPatchesLayer()(inp)\n",
    "    x = tf.keras.layers.Dense(d_model)(patches)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(d_model), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "    x = PositionalEncodingLayer()(x)\n",
    "    \n",
    "    \n",
    "    # Attention Module\n",
    "    v2 = tf.keras.layers.Dense(nv*nv)(x)\n",
    "    v2 = tf.keras.layers.RNN(SpikingReLU(nv*nv), return_sequences=True, return_state=False, stateful=True)(v2)\n",
    "    q2 = tf.keras.layers.Dense(nv*nv)(x)\n",
    "    q2 = tf.keras.layers.RNN(SpikingReLU(nv*nv), return_sequences=True, return_state=False, stateful=True)(q2)\n",
    "    k2 = tf.keras.layers.Dense(nv*nv)(x)\n",
    "    k2 = tf.keras.layers.RNN(SpikingReLU(nv*nv), return_sequences=True, return_state=False, stateful=True)(k2)\n",
    "    \n",
    "#     v2 = tf.keras.layers.Dense(nv*nv, activation=\"relu\")(x)\n",
    "#     q2 = tf.keras.layers.Dense(nv*nv, activation=\"relu\")(x)\n",
    "#     k2 = tf.keras.layers.Dense(nv*nv, activation=\"relu\")(x)\n",
    "\n",
    "    v = tf.keras.layers.Reshape([l, nv, nv])(v2)\n",
    "    q = tf.keras.layers.Reshape([l, nv, nv])(q2)\n",
    "    k = tf.keras.layers.Reshape([l, nv, nv])(k2)\n",
    "\n",
    "    # =============== Scaled dot-product attention =================\n",
    "    # QK^T\n",
    "    att = MatMulLayerTranspose()([q, k])\n",
    "    # softmax(QK^T)\n",
    "    # Exchanged: Softmax = RNN(Accumulate) + Softmax (Added Reshape layers around RNN layer)\n",
    "    att= tf.keras.layers.Reshape([l, nv*nv])(att)\n",
    "    att = tf.keras.layers.RNN(Accumulate(64), return_sequences=True, return_state=False, stateful=True)(att)\n",
    "    att= tf.keras.layers.Reshape([l, nv, nv])(att)\n",
    "    att = tf.keras.layers.Softmax(axis=-1)(att)\n",
    "    # softmax(QK^T)*V\n",
    "    out = MatMulLayer()([att, v])\n",
    "\n",
    "    out = tf.keras.layers.Reshape([l, d_model, 1])(out)\n",
    "    x = tf.keras.layers.Reshape([l, d_model, 1])(x)\n",
    "\n",
    "    # Concat Layer\n",
    "    add = tf.keras.layers.Add()([out, x])\n",
    "    out = tf.keras.layers.Flatten()(add)\n",
    "    out = tf.keras.layers.Dense(dout)(out)\n",
    "    out = tf.expand_dims(out, axis=1)\n",
    "    out = tf.keras.layers.RNN(SpikingReLU(dout), return_sequences=True, return_state=False, stateful=True)(out)\n",
    "    out = tf.keras.layers.Dense(num_classes)(out)\n",
    "    # Exchanged: Softmax = RNN(Accumulate) + Softmax\n",
    "    out = tf.keras.layers.RNN(Accumulate(num_classes), return_sequences=True, return_state=False, stateful=True)(out)\n",
    "    out = tf.keras.layers.Softmax()(out)\n",
    "    out = tf.squeeze(out, axis=1)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.set_weights(weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_weights(model, x_test, percentile=99.9):\n",
    "    x_test = x_test[::25]\n",
    "#     x_test = tf.squeeze(x_test, axis=1)\n",
    "    max_activation = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.ReLU):\n",
    "            activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "            if np.percentile(activation, percentile) > max_activation:\n",
    "                max_activation = np.percentile(activation, percentile)\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            if layer.activation.__name__ == 'relu':\n",
    "                activation = tf.keras.Model(inputs=model.inputs, outputs=layer.output)(x_test).numpy()\n",
    "                if np.percentile(activation, percentile) > max_activation:\n",
    "                    max_activation = np.percentile(activation, percentile)\n",
    "\n",
    "    weights = model.get_weights()     \n",
    "    if max_activation == 0:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNo normalization\\n\" + \"-\"*32)\n",
    "    else:\n",
    "        print(\"\\n\" + \"-\"*32 + \"\\nNormalizing by\", max_activation, \"\\n\" + \"-\"*32)\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] /= (max_activation)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_layer(layer_in, layer_out, x, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = 10\n",
    "\n",
    "    if len(x) % batch_size != 0:\n",
    "        x = x[: -(len(x) % batch_size)]\n",
    "\n",
    "    return Model(layer_in, layer_out).predict(x, batch_size)\n",
    "\n",
    "def weight_conversion_robust_and_data_based(weights, bias, model, data, normalization_method='robust', ppercentile=0.99):\n",
    "\n",
    "    if normalization_method == 'data':\n",
    "        ppercentile = 1.0\n",
    "\n",
    "    # Get weights from trained network\n",
    "    converted_weights = weights\n",
    "    converted_bias = bias\n",
    "    \n",
    "    # use training set to find max_act for each neuron\n",
    "            \n",
    "    activations = []\n",
    "    for l in range(0, len(converted_weights)):\n",
    "        activation = get_activations_layer(model.input, model.layers[l].output, data)\n",
    "        activation_per_neuron = [np.max(activation[:, i]) for i in range(activation.shape[1])]\n",
    "        activations.append(activation_per_neuron)\n",
    "        \n",
    "    previous_factor = 1\n",
    "    for l in range(len(converted_weights)):\n",
    "        # get the p-percentile of the activation\n",
    "        pos_inputs = activations[l]\n",
    "        pos_inputs.sort()\n",
    "        max_act = pos_inputs[int(ppercentile * (len(pos_inputs) - 1))]\n",
    "        # get the maximum weight in the layer\n",
    "        max_wt = tf.math.reduce_max(converted_weights[l])\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            max_bias = tf.math.reduce_max(converted_bias[l])\n",
    "            max_wt = tf.math.maximum(max_wt, max_bias)\n",
    "        scale_factor = tf.math.maximum(max_wt, max_act)\n",
    "\n",
    "        applied_factor = scale_factor / previous_factor\n",
    "        # rescale weights\n",
    "        converted_weights[l] = converted_weights[l] / applied_factor\n",
    "\n",
    "        # rescale bias\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            converted_bias[l] = converted_bias[l] / scale_factor\n",
    "        previous_factor = scale_factor\n",
    "        print(f\"Scale factor for this layer is {previous_factor}\")\n",
    "\n",
    "    return converted_weights, converted_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-179-be43693543a5>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  weights_ = np.array([model_weights[0], model_weights[1]])\n",
      "<ipython-input-179-be43693543a5>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  bias = np.array([model_weights[1], model_weights[3]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale factor for this layer is 1.0\n",
      "Scale factor for this layer is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 64])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights = get_normalized_weights(model, x_train, percentile=50)\n",
    "# weights = model.get_weights()\n",
    "model_weights = model.get_weights()\n",
    "weights_ = np.array([model_weights[0], model_weights[1]])\n",
    "bias = np.array([model_weights[1], model_weights[3]])\n",
    "weights, bias = weight_conversion_robust_and_data_based(weights_, bias, model, x_test)\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "# x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "# x_test = np.expand_dims(x_test, axis=1)\n",
    "#x_rnn = np.tile(x_train, (1, 1, 1))\n",
    "#y_rnn = y_train  # np.tile(x_test, (1, timesteps, 1))\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "# _, testacc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# snn = converted_model(weights)\n",
    "# evaluate_conversion(snn, model, x_test, y_test, testacc, timesteps=50)\n",
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LayerNormalization,\n",
    ")\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(\n",
    "            x, (batch_size, -1, self.num_heads, self.projection_dim)\n",
    "        )\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.mlp = tf.keras.Sequential(\n",
    "            [\n",
    "                Dense(mlp_dim, activation=tfa.activations.gelu),\n",
    "                Dropout(dropout),\n",
    "                Dense(embed_dim),\n",
    "                Dropout(dropout),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inputs_norm = self.layernorm1(inputs)\n",
    "        attn_output = self.att(inputs_norm)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = attn_output + inputs\n",
    "\n",
    "        out1_norm = self.layernorm2(out1)\n",
    "        mlp_output = self.mlp(out1_norm)\n",
    "        mlp_output = self.dropout2(mlp_output, training=training)\n",
    "        return mlp_output + out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize input so we can train ANN with it.\n",
    "# Will be converted back to integers for SNN layer.\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Add a channel dimension.\n",
    "axis = 1 if tf.keras.backend.image_data_format() == 'channels_first' else -1\n",
    "x_train = tf.expand_dims(x_train, axis)\n",
    "x_test = tf.expand_dims(x_test, axis)\n",
    "\n",
    "# One-hot encode target vectors.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiking Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LayerNormalization,\n",
    "    RNN\n",
    ")\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "class SpikingMultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(\n",
    "            x, (batch_size, -1, self.num_heads, self.projection_dim)\n",
    "        )\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SpikingTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = SpikingMultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.mlp = tf.keras.Sequential(\n",
    "            [\n",
    "                Dense(mlp_dim),\n",
    "                RNN(SpikingReLU(mlp_dim), return_sequences=True, return_state=False, stateful=True),\n",
    "                Dropout(dropout),\n",
    "                Dense(embed_dim),\n",
    "                Dropout(dropout),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inputs_norm = self.layernorm1(inputs)\n",
    "        attn_output = self.att(inputs_norm)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = attn_output + inputs\n",
    "\n",
    "        out1_norm = self.layernorm2(out1)\n",
    "        mlp_output = self.mlp(out1_norm)\n",
    "        mlp_output = self.dropout2(mlp_output, training=training)\n",
    "        return mlp_output + out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.__operators__.add_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'pos_emb:0' shape=(1, 50, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.__operators__.add_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'pos_emb:0' shape=(1, 50, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "/home/viktor/.local/lib/python3.8/site-packages/tensorflow_addons/utils/resource_loader.py:72: UserWarning: You are currently using TensorFlow 2.4.1 and trying to load a custom op (custom_ops/activations/_activation_ops.so).\n",
      "TensorFlow Addons has compiled its custom ops against TensorFlow 2.2.0, and there are no compatibility guarantees between the two versions. \n",
      "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
      " If you do, do not file an issue on Github. This is a known limitation.\n",
      "\n",
      "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
      "\n",
      "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.2.0 and strictly below 2.3.0.\n",
      " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
      "\n",
      "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/viktor/.local/lib/python3.8/site-packages/tensorflow_addons/options.py:47: RuntimeWarning: Traceback (most recent call last):\n",
      "  File \"/home/viktor/.local/lib/python3.8/site-packages/tensorflow_addons/activations/gelu.py\", line 56, in gelu\n",
      "    return _gelu_custom_op(x, approximate)\n",
      "  File \"/home/viktor/.local/lib/python3.8/site-packages/tensorflow_addons/activations/gelu.py\", line 71, in _gelu_custom_op\n",
      "    return _activation_so.ops.addons_gelu(x, approximate)\n",
      "  File \"/home/viktor/.local/lib/python3.8/site-packages/tensorflow_addons/utils/resource_loader.py\", line 64, in ops\n",
      "    self._ops = tf.load_op_library(get_path_to_datafile(self.relative_path))\n",
      "  File \"/home/viktor/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 57, in load_op_library\n",
      "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: /home/viktor/.local/lib/python3.8/site-packages/tensorflow_addons/custom_ops/activations/_activation_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\n",
      "\n",
      "\n",
      "The gelu C++/CUDA custom op could not be loaded.\n",
      "For this reason, Addons will fallback to an implementation written\n",
      "in Python with public TensorFlow ops. There worst you might experience with\n",
      "this is a moderate slowdown on GPU. There can be multiple\n",
      "reason for this loading error, one of them may be an ABI incompatibility between\n",
      "the TensorFlow installed on your system and the TensorFlow used to compile\n",
      "TensorFlow Addons' custom ops. The stacktrace generated when loading the\n",
      "shared object file was displayed above.\n",
      "\n",
      "If you want this warning to disappear, either make sure the TensorFlow installed\n",
      "is compatible with this version of Addons, or tell TensorFlow Addons to\n",
      "prefer using Python implementations and not custom C++/CUDA ones. You can do that\n",
      "by changing the TF_ADDONS_PY_OPS flag\n",
      "either with the environment variable:\n",
      "```bash\n",
      "TF_ADDONS_PY_OPS=1 python my_script.py\n",
      "```\n",
      "or in your code, after your imports:\n",
      "```python\n",
      "import tensorflow_addons as tfa\n",
      "import ...\n",
      "import ...\n",
      "\n",
      "tfa.options.TF_ADDONS_PY_OPS = True\n",
      "```\n",
      "\n",
      "  warnings.warn(warning_msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 21s 17ms/step - loss: 0.9392 - accuracy: 0.6778 - val_loss: 0.2894 - val_accuracy: 0.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5afdd38e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "dropout = 0.1\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "def extract_patches(images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dim])\n",
    "    return patches\n",
    "\n",
    "inp = tf.keras.layers.Input(shape=(input_shape))\n",
    "x = Rescaling(1.0 / 255)(inp)\n",
    "\n",
    "# =============== VISION PART =====================\n",
    "# patching, positional embedding and class embedding\n",
    "patches = extract_patches(x)\n",
    "x = Dense(d_model)(patches)\n",
    "\n",
    "pos_emb = tf.Variable(initial_value=tf.random.uniform(shape=(1, num_patches + 1, d_model)), \n",
    "                      name=\"pos_emb\", validate_shape=(1, num_patches + 1, d_model), trainable=True)\n",
    "class_emb = tf.Variable(initial_value=tf.random.uniform(shape=(1, 1, d_model)), name=\"class_emb\", \n",
    "                        validate_shape=(1, 1, d_model), trainable=True)\n",
    "\n",
    "class_emb = tf.broadcast_to(class_emb, [batch_size, 1, d_model])\n",
    "\n",
    "x = tf.concat([class_emb, x], axis=1)\n",
    "x = x + pos_emb\n",
    "\n",
    "# Transformer Blocks\n",
    "x = TransformerBlock(d_model, num_heads, mlp_dim, dropout)(x)\n",
    "\n",
    "# ================= MLP HEAD ===================\n",
    "x = Dense(mlp_dim, activation=tf.nn.relu)(x[:, 0])\n",
    "x = Dense(num_classes)(x)\n",
    "\n",
    "# ================ Model compilation and training ==================\n",
    "model = tf.keras.models.Model(inputs=inp, outputs=x)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "num_heads = 4\n",
    "mlp_dim = 128\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "def convert(model, weights, x_test, y_test):\n",
    "    print(\"Converted model:\\n\" + \"-\"*32)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "            print(\"Input Layer\")\n",
    "            inputs = tf.keras.Input(shape=(1, model.layers[0].input_shape[0][1]), batch_size=y_test.shape[0])\n",
    "            x = inputs        \n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            x = tf.keras.layers.Dense(layer.output_shape[1])(x)\n",
    "            # x = tf.keras.layers.RNN(DenseRNN(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            if layer.activation.__name__ == 'linear':\n",
    "                print(\"Dense Layer w/o activation\")\n",
    "                pass\n",
    "            elif layer.activation.__name__ == 'relu':\n",
    "                print(\"Dense Layer with SpikingReLU\")\n",
    "                x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'sigmoid':\n",
    "                print(\"Dense Layer with SpikingSigmoid\")\n",
    "                x = tf.keras.layers.RNN(SpikingSigmoid(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            elif layer.activation.__name__ == 'tanh':\n",
    "                print(\"Dense Layer with SpikingTanh\")\n",
    "                x = tf.keras.layers.RNN(SpikingTanh(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            else:\n",
    "                print('[Info] Activation type', layer.activation.__name__, 'not implemented')\n",
    "        elif isinstance(layer, tf.keras.layers.ReLU):\n",
    "            print(\"SpikingReLU Layer\")\n",
    "            x = tf.keras.layers.RNN(SpikingReLU(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "        elif isinstance(layer, tf.keras.layers.Softmax):\n",
    "            print(\"Accumulate + Softmax Layer\")\n",
    "            print(layer.output_shape[1])\n",
    "            x = tf.keras.layers.RNN(Accumulate(layer.output_shape[1]), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "            x = tf.keras.layers.Softmax()(x)\n",
    "        elif isinstance(layer, TransformerBlock):\n",
    "            x  = SpikingTransformerBlock(d_model, num_heads, mlp_dim, dropout)\n",
    "        else:\n",
    "            print(\"[Info] Layer type \", layer, \"not implemented, so we use it's keras version\")\n",
    "            x = layer(x)\n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "\n",
    "    spiking.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],)\n",
    "\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Dimensions of inputs should match: shape[0] = [50,1,64] vs. shape[1] = [60000,49,64] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fad6126523a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_normalized_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Preprocessing for RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (60000, 784) -> (60000, 1, 784)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4b8b1471280f>\u001b[0m in \u001b[0;36mget_normalized_weights\u001b[0;34m(model, x_test, percentile)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_activation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mmax_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \"\"\"\n\u001b[0;32m--> 424\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    425\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m_call_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;31m# Decorate the function to produce this layer's call method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_call_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m_call_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;31m# multiple ops w/ the same name when the layer is reused)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreated_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatched_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1675\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1676\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1677\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [50,1,64] vs. shape[1] = [60000,49,64] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "weights = get_normalized_weights(model, x_train, percentile=85)\n",
    "\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "#x_rnn = np.tile(x_train, (1, 1, 1))\n",
    "#y_rnn = y_train  # np.tile(x_test, (1, timesteps, 1))\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "snn = convert(ann, weights, x_test, y_test)\n",
    "evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
