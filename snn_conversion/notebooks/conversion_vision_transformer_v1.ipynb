{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from snn_conversion.spiking_models import SpikingReLU, Accumulate\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from snn_conversion.operations_layers import SqueezeLayer, ExpandLayer, ExtractPatchesLayer, PositionalEncodingLayer\n",
    "from snn_conversion.multi_head_self_attention import multi_head_self_attention\n",
    "from snn_conversion.old_normalization import get_normalized_weights\n",
    "from snn_conversion.utils import evaluate_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1238)\n",
    "batch_size=128\n",
    "epochs=2\n",
    "\n",
    "\n",
    "def create_ann_approved_version():\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "    patches = ExtractPatchesLayer()(inputs)\n",
    "    x = tf.keras.layers.Dense(d_model)(patches)\n",
    "    x = PositionalEncodingLayer()(x)\n",
    "    out = x\n",
    "    for i in range(1):\n",
    "        out, add = multi_head_self_attention(out)\n",
    "        out = tf.keras.layers.Dense(mlp_dim, activation=\"relu\")(add)\n",
    "        out = tf.keras.layers.Dense(embed_dim)(out)\n",
    "        out = tf.keras.layers.Add()([out, add])\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    \n",
    "    ann = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    ann.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    ann.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs)\n",
    "    return ann\n",
    "\n",
    "\n",
    "def convert_tailored_approved_version(weights, y_test):\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1), batch_size=y_test.shape[0])\n",
    "    \n",
    "    patches = ExtractPatchesLayer()(inputs)\n",
    "    x = tf.keras.layers.Dense(d_model)(patches)\n",
    "    x = PositionalEncodingLayer()(x)\n",
    "    out = x\n",
    "    for i in range(1):\n",
    "        out, add = multi_head_self_attention(out)\n",
    "        out = tf.keras.layers.Dense(mlp_dim)(add)\n",
    "        print(out.shape)\n",
    "        out = tf.keras.layers.Reshape([1, l*mlp_dim])(out)\n",
    "        out = tf.keras.layers.RNN(SpikingReLU(l*mlp_dim), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(out)\n",
    "        out = tf.keras.layers.Reshape([1, l, mlp_dim])(out)\n",
    "        \n",
    "        out = tf.keras.layers.Dense(embed_dim)(out)\n",
    "        out = tf.keras.layers.Add()([out, add])\n",
    "        \n",
    "    x = tf.keras.layers.Flatten()(out)\n",
    "    x = ExpandLayer()(x)\n",
    "    x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "    x = tf.keras.layers.RNN(SpikingReLU(embed_dim), return_sequences=True, return_state=False, \n",
    "                            stateful=True)(x)\n",
    "    # --------------------------------------------------\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "    \n",
    "    x = tf.keras.layers.RNN(Accumulate(num_classes), return_sequences=True, return_state=False, stateful=True)(x)\n",
    "    x = tf.keras.layers.Softmax()(x)\n",
    "    \n",
    "    x = SqueezeLayer()(x)\n",
    "    \n",
    "    spiking = tf.keras.models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    print(\"-\"*32 + \"\\n\")\n",
    "    spiking.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    print(spiking.summary())\n",
    "    spiking.set_weights(weights)\n",
    "    return spiking\n",
    "\n",
    "dv = 24\n",
    "dout = 32\n",
    "nv = 8\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = d_model = 64  # Embedding size for each token\n",
    "mlp_dim = 128\n",
    "l = 50\n",
    "num_heads = 4\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "patch_size = 4\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "channels = 1\n",
    "patch_dim = channels * patch_size ** 2\n",
    "projection_dim = embed_dim//num_heads\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize input so we can train ANN with it.\n",
    "# Will be converted back to integers for SNN layer.\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Add a channel dimension.\n",
    "axis = 1 if keras.backend.image_data_format() == 'channels_first' else -1\n",
    "x_train = np.expand_dims(x_train, axis)\n",
    "x_test = np.expand_dims(x_test, axis)\n",
    "\n",
    "# One-hot encode target vectors.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Analog model\n",
    "ann = create_ann_approved_version()\n",
    "print(ann.summary())\n",
    "\n",
    "_, testacc = ann.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "# weights = ann.get_weights()\n",
    "weights = get_normalized_weights(ann, x_train, percentile=85)\n",
    "\n",
    "##################################################\n",
    "# Preprocessing for RNN \n",
    "# x_train = np.expand_dims(x_train, axis=1)  # (60000, 784) -> (60000, 1, 784)\n",
    "# x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "##################################################\n",
    "# Conversion to spiking model\n",
    "# snn = convert(ann, weights, x_test, y_test)\n",
    "snn = convert_tailored_approved_version(weights, y_test)\n",
    "evaluate_conversion(snn, ann, x_test, y_test, testacc, timesteps=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}