{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Transformer model for sequential MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:23:45.920611Z",
     "iopub.status.busy": "2020-11-02T02:23:45.919935Z",
     "iopub.status.idle": "2020-11-02T02:23:53.342882Z",
     "shell.execute_reply": "2020-11-02T02:23:53.342273Z"
    },
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd1NWMxjfsDd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup input MNIST pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "tf.Tensor([7], shape=(1,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 40\n",
    "EPOCHS = 20\n",
    "\n",
    "train_mnist, test_mnist = tfds.load('mnist', split=['train', 'test'], shuffle_files=True)\n",
    "\n",
    "# Build your input pipeline\n",
    "train_mnist = train_mnist.shuffle(1024).repeat().batch(1)\n",
    "for example in train_mnist.take(1):\n",
    "    image, label = example['image'], example['label']\n",
    "    print(image.shape)\n",
    "    print(label)\n",
    "\n",
    "for i in image:\n",
    "    plt.imshow(tf.squeeze(i))\n",
    "    plt.show()\n",
    "\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_mnist_prefetch = train_mnist.cache()\n",
    "train_mnist_prefetch = train_mnist_prefetch.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_mnist_prefetch = train_mnist_prefetch.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> ======================= TEST =====================</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = next(iter(test_mnist))\n",
    "pooling = tf.keras.layers.MaxPool2D()\n",
    "data_expanded = tf.expand_dims(\n",
    "   data['image'] , 0, name=None\n",
    ")\n",
    "\n",
    "data_expanded_pooled = pooling(data_expanded)\n",
    "plt.imshow(tf.squeeze(data_expanded_pooled))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(tf.squeeze(data['image']))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> ============================================</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:23:53.350740Z",
     "iopub.status.busy": "2020-11-02T02:23:53.349965Z",
     "iopub.status.idle": "2020-11-02T02:24:16.829151Z",
     "shell.execute_reply": "2020-11-02T02:24:16.829598Z"
    },
    "id": "8q9t4FmN96eN"
   },
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .',\n",
       " b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(train_examples))\n",
    "pt_batch.numpy(), en_batch.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCEKotqosGfq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a custom subwords tokenizer from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:24:16.835394Z",
     "iopub.status.busy": "2020-11-02T02:24:16.834726Z",
     "iopub.status.idle": "2020-11-02T02:26:24.694629Z",
     "shell.execute_reply": "2020-11-02T02:26:24.695136Z"
    },
    "id": "KVBg5Q8tBk5z"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> ======================= TEST for Speech =====================</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.700665Z",
     "iopub.status.busy": "2020-11-02T02:26:24.700009Z",
     "iopub.status.idle": "2020-11-02T02:26:24.702248Z",
     "shell.execute_reply": "2020-11-02T02:26:24.702679Z"
    },
    "id": "4DYWukNFkGQN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9KJWJjrsZ4Y"
   },
   "source": [
    "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.707214Z",
     "iopub.status.busy": "2020-11-02T02:26:24.706529Z",
     "iopub.status.idle": "2020-11-02T02:26:24.708820Z",
     "shell.execute_reply": "2020-11-02T02:26:24.709240Z"
    },
    "id": "bf2ntBxjkqK6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7915 ----> T\n",
      "1248 ----> ran\n",
      "7946 ----> s\n",
      "7194 ----> former \n",
      "13 ----> is \n",
      "2799 ----> awesome\n",
      "7877 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> ============================================</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.712950Z",
     "iopub.status.busy": "2020-11-02T02:26:24.712287Z",
     "iopub.status.idle": "2020-11-02T02:26:24.714005Z",
     "shell.execute_reply": "2020-11-02T02:26:24.714450Z"
    },
    "id": "bcRp7VcQ5m6g"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGi4PoVakxdc"
   },
   "source": [
    "Add a start and end token to the input and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.719434Z",
     "iopub.status.busy": "2020-11-02T02:26:24.718759Z",
     "iopub.status.idle": "2020-11-02T02:26:24.720965Z",
     "shell.execute_reply": "2020-11-02T02:26:24.720470Z"
    },
    "id": "UZwnPr4R055s"
   },
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx1sFbR-9fRs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
    "\n",
    "* Graph tensors do not have a value. \n",
    "* In graph mode you can only use TensorFlow Ops and functions. \n",
    "\n",
    "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.725590Z",
     "iopub.status.busy": "2020-11-02T02:26:24.724939Z",
     "iopub.status.idle": "2020-11-02T02:26:24.726798Z",
     "shell.execute_reply": "2020-11-02T02:26:24.727223Z"
    },
    "id": "Mah1cS-P70Iz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JrGp5Gek6Ql",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.731110Z",
     "iopub.status.busy": "2020-11-02T02:26:24.730465Z",
     "iopub.status.idle": "2020-11-02T02:26:24.732249Z",
     "shell.execute_reply": "2020-11-02T02:26:24.732655Z"
    },
    "id": "2QEgbjntk6Yf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.736845Z",
     "iopub.status.busy": "2020-11-02T02:26:24.736227Z",
     "iopub.status.idle": "2020-11-02T02:26:24.737975Z",
     "shell.execute_reply": "2020-11-02T02:26:24.738396Z"
    },
    "id": "c081xPGv1CPI"
   },
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.743511Z",
     "iopub.status.busy": "2020-11-02T02:26:24.742805Z",
     "iopub.status.idle": "2020-11-02T02:26:24.863582Z",
     "shell.execute_reply": "2020-11-02T02:26:24.862990Z"
    },
    "id": "9mk9AZdZ5bcS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.867883Z",
     "iopub.status.busy": "2020-11-02T02:26:24.867130Z",
     "iopub.status.idle": "2020-11-02T02:26:24.945263Z",
     "shell.execute_reply": "2020-11-02T02:26:24.945694Z"
    },
    "id": "_fXvfYVfQr2n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
       " array([[8214,  342, 3032, ...,    0,    0,    0],\n",
       "        [8214,   95,  198, ...,    0,    0,    0],\n",
       "        [8214, 4479, 7990, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,  584,   12, ...,    0,    0,    0],\n",
       "        [8214,   59, 1548, ...,    0,    0,    0],\n",
       "        [8214,  118,   34, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8087,   98,   25, ...,    0,    0,    0],\n",
       "        [8087,   12,   20, ...,    0,    0,    0],\n",
       "        [8087,   12, 5453, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,   18, 2059, ...,    0,    0,    0],\n",
       "        [8087,   16, 1436, ...,    0,    0,    0],\n",
       "        [8087,   15,   57, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBQuibYA4n0n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.950742Z",
     "iopub.status.busy": "2020-11-02T02:26:24.949856Z",
     "iopub.status.idle": "2020-11-02T02:26:24.951933Z",
     "shell.execute_reply": "2020-11-02T02:26:24.952383Z"
    },
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.958548Z",
     "iopub.status.busy": "2020-11-02T02:26:24.957602Z",
     "iopub.status.idle": "2020-11-02T02:26:24.960152Z",
     "shell.execute_reply": "2020-11-02T02:26:24.959567Z"
    },
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:24.964937Z",
     "iopub.status.busy": "2020-11-02T02:26:24.964299Z",
     "iopub.status.idle": "2020-11-02T02:26:25.185963Z",
     "shell.execute_reply": "2020-11-02T02:26:25.186470Z"
    },
    "id": "1kLCla68EloE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_b4ou4TYqUN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s42Uydjkv0hF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.191500Z",
     "iopub.status.busy": "2020-11-02T02:26:25.190865Z",
     "iopub.status.idle": "2020-11-02T02:26:25.192659Z",
     "shell.execute_reply": "2020-11-02T02:26:25.193052Z"
    },
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.197206Z",
     "iopub.status.busy": "2020-11-02T02:26:25.196569Z",
     "iopub.status.idle": "2020-11-02T02:26:25.201194Z",
     "shell.execute_reply": "2020-11-02T02:26:25.201613Z"
    },
    "id": "A7BYeBCNvi7n",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.206009Z",
     "iopub.status.busy": "2020-11-02T02:26:25.205344Z",
     "iopub.status.idle": "2020-11-02T02:26:25.207194Z",
     "shell.execute_reply": "2020-11-02T02:26:25.207586Z"
    },
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.211387Z",
     "iopub.status.busy": "2020-11-02T02:26:25.210780Z",
     "iopub.status.idle": "2020-11-02T02:26:25.214859Z",
     "shell.execute_reply": "2020-11-02T02:26:25.215313Z"
    },
    "id": "yxKGuXxaBeeE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsxEE_-Wa1gF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.222000Z",
     "iopub.status.busy": "2020-11-02T02:26:25.221312Z",
     "iopub.status.idle": "2020-11-02T02:26:25.223205Z",
     "shell.execute_reply": "2020-11-02T02:26:25.223597Z"
    },
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.228177Z",
     "iopub.status.busy": "2020-11-02T02:26:25.227454Z",
     "iopub.status.idle": "2020-11-02T02:26:25.229676Z",
     "shell.execute_reply": "2020-11-02T02:26:25.229174Z"
    },
    "id": "n90YjClyInFy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.235475Z",
     "iopub.status.busy": "2020-11-02T02:26:25.234773Z",
     "iopub.status.idle": "2020-11-02T02:26:25.238644Z",
     "shell.execute_reply": "2020-11-02T02:26:25.239081Z"
    },
    "id": "yAzUAf2DPlNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.243123Z",
     "iopub.status.busy": "2020-11-02T02:26:25.242423Z",
     "iopub.status.idle": "2020-11-02T02:26:25.246814Z",
     "shell.execute_reply": "2020-11-02T02:26:25.246309Z"
    },
    "id": "zg6k-fGhgXra",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.250955Z",
     "iopub.status.busy": "2020-11-02T02:26:25.250254Z",
     "iopub.status.idle": "2020-11-02T02:26:25.253722Z",
     "shell.execute_reply": "2020-11-02T02:26:25.254132Z"
    },
    "id": "UAq3YOzUgXhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOz-4_XIhaTP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.258596Z",
     "iopub.status.busy": "2020-11-02T02:26:25.257867Z",
     "iopub.status.idle": "2020-11-02T02:26:25.307528Z",
     "shell.execute_reply": "2020-11-02T02:26:25.307944Z"
    },
    "id": "6dlU8Tm-hYrF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmzGPEy64qmA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz5BMC8Kaoqo"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPmbr6F1C-v_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.319284Z",
     "iopub.status.busy": "2020-11-02T02:26:25.318566Z",
     "iopub.status.idle": "2020-11-02T02:26:25.320448Z",
     "shell.execute_reply": "2020-11-02T02:26:25.320855Z"
    },
    "id": "BSV3PPKsYecw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.326077Z",
     "iopub.status.busy": "2020-11-02T02:26:25.325340Z",
     "iopub.status.idle": "2020-11-02T02:26:25.359646Z",
     "shell.execute_reply": "2020-11-02T02:26:25.360072Z"
    },
    "id": "Hu94p-_-2_BX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdDqGayx67vv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.364864Z",
     "iopub.status.busy": "2020-11-02T02:26:25.364204Z",
     "iopub.status.idle": "2020-11-02T02:26:25.366219Z",
     "shell.execute_reply": "2020-11-02T02:26:25.366676Z"
    },
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.373368Z",
     "iopub.status.busy": "2020-11-02T02:26:25.372687Z",
     "iopub.status.idle": "2020-11-02T02:26:25.437666Z",
     "shell.execute_reply": "2020-11-02T02:26:25.437066Z"
    },
    "id": "mytb1lPyOHLB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yScbC0MUH8dS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfYJG-Kvgwy2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.446072Z",
     "iopub.status.busy": "2020-11-02T02:26:25.445347Z",
     "iopub.status.idle": "2020-11-02T02:26:25.447785Z",
     "shell.execute_reply": "2020-11-02T02:26:25.447205Z"
    },
    "id": "ncyS-Ms3i2x_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.459795Z",
     "iopub.status.busy": "2020-11-02T02:26:25.459178Z",
     "iopub.status.idle": "2020-11-02T02:26:25.582504Z",
     "shell.execute_reply": "2020-11-02T02:26:25.582945Z"
    },
    "id": "AzZRXdO0mI48",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.592814Z",
     "iopub.status.busy": "2020-11-02T02:26:25.592125Z",
     "iopub.status.idle": "2020-11-02T02:26:25.593830Z",
     "shell.execute_reply": "2020-11-02T02:26:25.594250Z"
    },
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.607195Z",
     "iopub.status.busy": "2020-11-02T02:26:25.606534Z",
     "iopub.status.idle": "2020-11-02T02:26:25.804635Z",
     "shell.execute_reply": "2020-11-02T02:26:25.805053Z"
    },
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE1H51Ajm0q1"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.814730Z",
     "iopub.status.busy": "2020-11-02T02:26:25.814013Z",
     "iopub.status.idle": "2020-11-02T02:26:25.816267Z",
     "shell.execute_reply": "2020-11-02T02:26:25.815698Z"
    },
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.pooling = tf.keras.layers.MaxPool2D()\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.pooling(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:25.824870Z",
     "iopub.status.busy": "2020-11-02T02:26:25.824233Z",
     "iopub.status.idle": "2020-11-02T02:26:26.357891Z",
     "shell.execute_reply": "2020-11-02T02:26:26.358338Z"
    },
    "id": "8QG9nueFQKXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[222  55  76 175 141 108  41 216 200  33  82  63  60 172  44 123   8\n",
      "      9 217  14 236  75 229 132  59 128  82 151]\n",
      "   [242 102  76 222  70 147  78 111 200   6  21  84 157  35 154  43 138\n",
      "    149 111 128 251 122 197   3 218 212  35  11]\n",
      "   [ 76 175 226 101 197 132 234  32  21 144 201 131 134 112 144 232 135\n",
      "      6 227 209  61  60 117  81  69 165 133  86]\n",
      "   [115  71  63   0  23 210 194 163  17  43  61 249 106  56 110 215 181\n",
      "    111 237 136  40  61 166 102  40 204 219 127]\n",
      "   [191  83  13 191  56 111 235 103  53  82 235 131  32 234 224  24  62\n",
      "      5 235  51 201 181  12 162  12  89  65  45]\n",
      "   [111  16 194 191 122   9 208  18 105 196  29   1 222 118 193  61 149\n",
      "     98 186 136 200  68  61  71  20   2 152 136]\n",
      "   [105 123 248 236 188  88  87  26   5  91  52   7 181 105  92  26 135\n",
      "    163 198 196  64  53   3 228 224  70 191 230]\n",
      "   [119 177   3 156 183  87  34  71  63  56  47  64 185 146 158  39 185\n",
      "    119  75 134  27 232 213   1 179 169  80 120]\n",
      "   [ 69 144 136 157 170 197 168 233 180 154 197 110  43 225 219  28  17\n",
      "    242 122 131 168 137 222  70 163 202 245 163]\n",
      "   [ 79   7  28 175  47  67 162  49 103 221  97 137  99 160 225 146 172\n",
      "     72 136 127 133 209 107  56 102  94  43  74]\n",
      "   [ 41 226 154 199 170 237 163 183 172 146 216 172 136  88 170  59 120\n",
      "     34 171  61   8  94   6 243 150  88 173  43]\n",
      "   [ 17 203 103  60 167  31  94 107 158 133 118 246  73 159   2 254 190\n",
      "    184 224 121 165 129 246  49  63  35  78  88]\n",
      "   [ 40  60 125 234 118 112  39 114  81 161 249 110  99  21  30 102 143\n",
      "    203 250  68 149  90 229  34 176  23 240  89]\n",
      "   [ 38 183 114 248  82 168 157  93  14  83 230 237 175 232 218  53  35\n",
      "    182 255 149  71 120  11  92  62 122 159   2]\n",
      "   [ 41  89 178 218  46 210 170 146 224  13  82 209 213  17 225 245 109\n",
      "     84 226 234 194  49 167 144 172  87  30 150]\n",
      "   [ 29 143 187 140 193 140 173 191  12 184 128  98  34  32  18  68 237\n",
      "    220 238  82 115  93 212 238 230  33 162 253]\n",
      "   [218 226  25  98 251 168 194 241 251  25 164  17  65  86  81 151 220\n",
      "    240 135 237  83  47 251  45  42 162  44  33]\n",
      "   [ 27   4 111 171  50 172 195 107 122  90 170  19  73 217  55  90 107\n",
      "    245  90 136  95  75  31  14  94 127  87 128]\n",
      "   [ 11 151 191  95 103  74 123 156 155 227  76 182  10 255 238 180  93\n",
      "     22 150 144 113  42 241  48  30  36  91  64]\n",
      "   [134 122 127 139 111  42 186 150 122 183 241  16 121  48  72  67  48\n",
      "    119 101  28 204 206 124  42  51  61 236 252]\n",
      "   [ 88 140 248  74 176  68  17  55 223 239 209  69 105   9  95 139  40\n",
      "    121  12  79  31 124  88 123 223 238  79 119]\n",
      "   [152 164 235 224 120  84 223  35 240 149 117 133 159   0 196 153  92\n",
      "    230 197 198  83 113  60 248 133 111 188   8]\n",
      "   [243 109  81  25  78 208 160 206  91 134 228 248  28 226 161  97 240\n",
      "    223 220 138  78 109 152 152  75  50 205  44]\n",
      "   [219 101  83  61  82  27  86 124 188  12   6 147 255  23 126 182  25\n",
      "    229 202 155  84  88 205  45 233  48 115 172]\n",
      "   [ 44  64 246 156 107   2  61  82 151  51  93 125 104  68  25 169  97\n",
      "    116 170 108  48  49  17  49 227  68  47 107]\n",
      "   [100 115 226  67  68  88 190 112  84  62  48 236 132 152 223  15 237\n",
      "    219  13   7 104 116 140 131  44 104 244 238]\n",
      "   [243 168 109 139  50 174 198 108 231 117 156 132 145  22 131 211  71\n",
      "    130 165  58 143 137  20 242  27  82 192 241]\n",
      "   [166  20 160 233  37 195 209 123  11  57 126 212  69 209 143 123  76\n",
      "    248  90 100  42 237 128 133 183 216 125 111]]]], shape=(1, 1, 28, 28), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((1, 1, 28, 28), dtype=tf.int64, minval=0, maxval=256)\n",
    "print(temp_input)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.cast(temp_input, tf.float32), training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtT7PKzrXkNr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:26.368605Z",
     "iopub.status.busy": "2020-11-02T02:26:26.367591Z",
     "iopub.status.idle": "2020-11-02T02:26:26.369596Z",
     "shell.execute_reply": "2020-11-02T02:26:26.369997Z"
    },
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.pooling = tf.keras.layers.MaxPool2D()\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.pooling(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:26.377767Z",
     "iopub.status.busy": "2020-11-02T02:26:26.376310Z",
     "iopub.status.idle": "2020-11-02T02:26:26.755762Z",
     "shell.execute_reply": "2020-11-02T02:26:26.755141Z"
    },
    "id": "a1jXoAMRZyvu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_encoder_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-94-5509d3a6b214>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m output, attn = sample_decoder(temp_input, \n\u001B[0;32m----> 7\u001B[0;31m                               \u001B[0menc_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msample_encoder_output\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m                               \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m                               \u001B[0mlook_ahead_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sample_encoder_output' is not defined"
     ]
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uERO1y54cOKq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:26.763467Z",
     "iopub.status.busy": "2020-11-02T02:26:26.762728Z",
     "iopub.status.idle": "2020-11-02T02:26:26.764477Z",
     "shell.execute_reply": "2020-11-02T02:26:26.764889Z"
    },
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:26.772608Z",
     "iopub.status.busy": "2020-11-02T02:26:26.771855Z",
     "iopub.status.idle": "2020-11-02T02:26:27.696061Z",
     "shell.execute_reply": "2020-11-02T02:26:27.696483Z"
    },
    "id": "tJ4fbQcIkHW1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVjWCxFNcgbt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.701337Z",
     "iopub.status.busy": "2020-11-02T02:26:27.700717Z",
     "iopub.status.idle": "2020-11-02T02:26:27.702463Z",
     "shell.execute_reply": "2020-11-02T02:26:27.702871Z"
    },
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.709107Z",
     "iopub.status.busy": "2020-11-02T02:26:27.708470Z",
     "iopub.status.idle": "2020-11-02T02:26:27.710049Z",
     "shell.execute_reply": "2020-11-02T02:26:27.710587Z"
    },
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.715008Z",
     "iopub.status.busy": "2020-11-02T02:26:27.714374Z",
     "iopub.status.idle": "2020-11-02T02:26:27.716089Z",
     "shell.execute_reply": "2020-11-02T02:26:27.716593Z"
    },
    "id": "7r4scdulztRx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.720844Z",
     "iopub.status.busy": "2020-11-02T02:26:27.720237Z",
     "iopub.status.idle": "2020-11-02T02:26:27.866519Z",
     "shell.execute_reply": "2020-11-02T02:26:27.865886Z"
    },
    "id": "f33ZCgvHpPdG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgkDE7hzo8r5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxGJtoDuYIHL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.870775Z",
     "iopub.status.busy": "2020-11-02T02:26:27.870132Z",
     "iopub.status.idle": "2020-11-02T02:26:27.871766Z",
     "shell.execute_reply": "2020-11-02T02:26:27.872206Z"
    },
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.879108Z",
     "iopub.status.busy": "2020-11-02T02:26:27.878422Z",
     "iopub.status.idle": "2020-11-02T02:26:27.880191Z",
     "shell.execute_reply": "2020-11-02T02:26:27.880573Z"
    },
    "id": "67oqVHiT0Eiu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "  \n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.890929Z",
     "iopub.status.busy": "2020-11-02T02:26:27.890255Z",
     "iopub.status.idle": "2020-11-02T02:26:27.898638Z",
     "shell.execute_reply": "2020-11-02T02:26:27.899073Z"
    },
    "id": "phlyxMnm-Tpx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeHumfr7zmMa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:27.904511Z",
     "iopub.status.busy": "2020-11-02T02:26:27.903860Z",
     "iopub.status.idle": "2020-11-02T02:26:28.059188Z",
     "shell.execute_reply": "2020-11-02T02:26:28.059633Z"
    },
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:28.065224Z",
     "iopub.status.busy": "2020-11-02T02:26:28.064487Z",
     "iopub.status.idle": "2020-11-02T02:26:28.066163Z",
     "shell.execute_reply": "2020-11-02T02:26:28.066631Z"
    },
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:28.072057Z",
     "iopub.status.busy": "2020-11-02T02:26:28.071394Z",
     "iopub.status.idle": "2020-11-02T02:26:28.073614Z",
     "shell.execute_reply": "2020-11-02T02:26:28.073034Z"
    },
    "id": "hNhuYfllndLZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Di_Yaa1gf9r",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peeking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:28.077441Z",
     "iopub.status.busy": "2020-11-02T02:26:28.076745Z",
     "iopub.status.idle": "2020-11-02T02:26:28.078903Z",
     "shell.execute_reply": "2020-11-02T02:26:28.078389Z"
    },
    "id": "LKpoA6q1sJFj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:28.086043Z",
     "iopub.status.busy": "2020-11-02T02:26:28.085376Z",
     "iopub.status.idle": "2020-11-02T02:26:28.087078Z",
     "shell.execute_reply": "2020-11-02T02:26:28.087521Z"
    },
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  print(inp)\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "#   enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM2PDWGDJ_8V"
   },
   "source": [
    "Portuguese is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T02:26:28.094989Z",
     "iopub.status.busy": "2020-11-02T02:26:28.094309Z",
     "iopub.status.idle": "2020-11-02T04:16:37.754510Z",
     "shell.execute_reply": "2020-11-02T04:16:37.753863Z"
    },
    "id": "bbvmaKNiznHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Input\n",
      "tf.Tensor(\n",
      "[[[[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 16]\n",
      "   [119]\n",
      "   [169]\n",
      "   [254]\n",
      "   [163]\n",
      "   [163]\n",
      "   [118]\n",
      "   [ 16]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 20]\n",
      "   [139]\n",
      "   [239]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [254]\n",
      "   [253]\n",
      "   [253]\n",
      "   [233]\n",
      "   [ 94]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 91]\n",
      "   [254]\n",
      "   [253]\n",
      "   [247]\n",
      "   [145]\n",
      "   [127]\n",
      "   [ 36]\n",
      "   [ 36]\n",
      "   [ 49]\n",
      "   [213]\n",
      "   [240]\n",
      "   [ 46]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [162]\n",
      "   [254]\n",
      "   [203]\n",
      "   [ 46]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 70]\n",
      "   [253]\n",
      "   [ 91]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [181]\n",
      "   [228]\n",
      "   [ 37]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  6]\n",
      "   [201]\n",
      "   [ 91]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  7]\n",
      "   [207]\n",
      "   [124]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 38]\n",
      "   [228]\n",
      "   [ 92]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [181]\n",
      "   [111]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 60]\n",
      "   [224]\n",
      "   [253]\n",
      "   [  7]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [168]\n",
      "   [217]\n",
      "   [ 35]\n",
      "   [  6]\n",
      "   [ 37]\n",
      "   [ 69]\n",
      "   [128]\n",
      "   [211]\n",
      "   [248]\n",
      "   [253]\n",
      "   [ 98]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 91]\n",
      "   [254]\n",
      "   [241]\n",
      "   [207]\n",
      "   [253]\n",
      "   [253]\n",
      "   [254]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [149]\n",
      "   [  7]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 26]\n",
      "   [221]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [254]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [149]\n",
      "   [  7]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  6]\n",
      "   [170]\n",
      "   [255]\n",
      "   [254]\n",
      "   [254]\n",
      "   [248]\n",
      "   [181]\n",
      "   [124]\n",
      "   [142]\n",
      "   [ 58]\n",
      "   [  7]\n",
      "   [124]\n",
      "   [255]\n",
      "   [109]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [123]\n",
      "   [253]\n",
      "   [238]\n",
      "   [ 95]\n",
      "   [ 18]\n",
      "   [ 17]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [169]\n",
      "   [223]\n",
      "   [ 21]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 19]\n",
      "   [226]\n",
      "   [240]\n",
      "   [ 58]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [118]\n",
      "   [253]\n",
      "   [ 36]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [108]\n",
      "   [251]\n",
      "   [119]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [112]\n",
      "   [253]\n",
      "   [ 36]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [128]\n",
      "   [235]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 33]\n",
      "   [228]\n",
      "   [253]\n",
      "   [ 36]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [128]\n",
      "   [235]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 64]\n",
      "   [228]\n",
      "   [255]\n",
      "   [140]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [128]\n",
      "   [251]\n",
      "   [138]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 51]\n",
      "   [145]\n",
      "   [249]\n",
      "   [253]\n",
      "   [192]\n",
      "   [ 14]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [109]\n",
      "   [245]\n",
      "   [253]\n",
      "   [192]\n",
      "   [108]\n",
      "   [ 37]\n",
      "   [ 43]\n",
      "   [128]\n",
      "   [186]\n",
      "   [250]\n",
      "   [253]\n",
      "   [250]\n",
      "   [151]\n",
      "   [ 13]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [114]\n",
      "   [215]\n",
      "   [254]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [253]\n",
      "   [254]\n",
      "   [253]\n",
      "   [159]\n",
      "   [ 69]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [ 59]\n",
      "   [189]\n",
      "   [201]\n",
      "   [240]\n",
      "   [253]\n",
      "   [253]\n",
      "   [163]\n",
      "   [ 78]\n",
      "   [ 11]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]\n",
      "\n",
      "  [[  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]\n",
      "   [  0]]]], shape=(1, 28, 28, 1), dtype=uint8)\n",
      "Target\n",
      "tf.Tensor([8], shape=(1,), dtype=int64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Python inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor(\n[[[[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 16]\n   [119]\n   [169]\n   [254]\n   [163]\n   [163]\n   [118]\n   [ 16]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 20]\n   [139]\n   [239]\n   [253]\n   [253]\n   [253]\n   [254]\n   [253]\n   [253]\n   [233]\n   [ 94]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 91]\n   [254]\n   [253]\n   [247]\n   [145]\n   [127]\n   [ 36]\n   [ 36]\n   [ 49]\n   [213]\n   [240]\n   [ 46]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [162]\n   [254]\n   [203]\n   [ 46]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 70]\n   [253]\n   [ 91]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [181]\n   [228]\n   [ 37]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  6]\n   [201]\n   [ 91]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  7]\n   [207]\n   [124]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 38]\n   [228]\n   [ 92]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [181]\n   [111]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 60]\n   [224]\n   [253]\n   [  7]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [168]\n   [217]\n   [ 35]\n   [  6]\n   [ 37]\n   [ 69]\n   [128]\n   [211]\n   [248]\n   [253]\n   [ 98]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 91]\n   [254]\n   [241]\n   [207]\n   [253]\n   [253]\n   [254]\n   [253]\n   [253]\n   [253]\n   [149]\n   [  7]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 26]\n   [221]\n   [253]\n   [253]\n   [253]\n   [253]\n   [254]\n   [253]\n   [253]\n   [253]\n   [253]\n   [149]\n   [  7]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  6]\n   [170]\n   [255]\n   [254]\n   [254]\n   [248]\n   [181]\n   [124]\n   [142]\n   [ 58]\n   [  7]\n   [124]\n   [255]\n   [109]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [123]\n   [253]\n   [238]\n   [ 95]\n   [ 18]\n   [ 17]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [169]\n   [223]\n   [ 21]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 19]\n   [226]\n   [240]\n   [ 58]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [118]\n   [253]\n   [ 36]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [108]\n   [251]\n   [119]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [112]\n   [253]\n   [ 36]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [128]\n   [235]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 33]\n   [228]\n   [253]\n   [ 36]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [128]\n   [235]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 64]\n   [228]\n   [255]\n   [140]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [128]\n   [251]\n   [138]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 51]\n   [145]\n   [249]\n   [253]\n   [192]\n   [ 14]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [109]\n   [245]\n   [253]\n   [192]\n   [108]\n   [ 37]\n   [ 43]\n   [128]\n   [186]\n   [250]\n   [253]\n   [250]\n   [151]\n   [ 13]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [114]\n   [215]\n   [254]\n   [253]\n   [253]\n   [253]\n   [253]\n   [254]\n   [253]\n   [159]\n   [ 69]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 59]\n   [189]\n   [201]\n   [240]\n   [253]\n   [253]\n   [163]\n   [ 78]\n   [ 11]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]]], shape=(1, 28, 28, 1), dtype=uint8),\n    tf.Tensor([8], shape=(1,), dtype=int64))\n  input_signature: (\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-123-82e0950eca3b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Target\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"label\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"image\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"label\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;36m50\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    778\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"nonXla\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 780\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    812\u001B[0m       \u001B[0;31m# In this case we have not created variables on the first call. So we can\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m       \u001B[0;31m# run the first trace but we should fail if variables are created.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 814\u001B[0;31m       \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    815\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_created_variables\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    816\u001B[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2826\u001B[0m     \u001B[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2827\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2828\u001B[0;31m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2829\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2830\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   3168\u001B[0m     \"\"\"\n\u001B[1;32m   3169\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput_signature\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0margs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3170\u001B[0;31m       args, kwargs = self._function_spec.canonicalize_function_inputs(\n\u001B[0m\u001B[1;32m   3171\u001B[0m           *args, **kwargs)\n\u001B[1;32m   3172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcanonicalize_function_inputs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2617\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2618\u001B[0m       \u001B[0;32massert\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2619\u001B[0;31m       inputs = _convert_inputs_to_signature(\n\u001B[0m\u001B[1;32m   2620\u001B[0m           \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2621\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_input_signature\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_convert_inputs_to_signature\u001B[0;34m(inputs, input_signature, flat_input_signature)\u001B[0m\n\u001B[1;32m   2710\u001B[0m       \u001B[0mflat_input_signature\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2711\u001B[0m       flatten_inputs)):\n\u001B[0;32m-> 2712\u001B[0;31m     raise ValueError(\"Python inputs incompatible with input_signature:\\n%s\" %\n\u001B[0m\u001B[1;32m   2713\u001B[0m                      format_error_message(inputs, input_signature))\n\u001B[1;32m   2714\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Python inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor(\n[[[[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 16]\n   [119]\n   [169]\n   [254]\n   [163]\n   [163]\n   [118]\n   [ 16]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 20]\n   [139]\n   [239]\n   [253]\n   [253]\n   [253]\n   [254]\n   [253]\n   [253]\n   [233]\n   [ 94]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 91]\n   [254]\n   [253]\n   [247]\n   [145]\n   [127]\n   [ 36]\n   [ 36]\n   [ 49]\n   [213]\n   [240]\n   [ 46]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [162]\n   [254]\n   [203]\n   [ 46]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 70]\n   [253]\n   [ 91]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [181]\n   [228]\n   [ 37]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  6]\n   [201]\n   [ 91]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  7]\n   [207]\n   [124]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 38]\n   [228]\n   [ 92]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [181]\n   [111]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 60]\n   [224]\n   [253]\n   [  7]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [168]\n   [217]\n   [ 35]\n   [  6]\n   [ 37]\n   [ 69]\n   [128]\n   [211]\n   [248]\n   [253]\n   [ 98]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 91]\n   [254]\n   [241]\n   [207]\n   [253]\n   [253]\n   [254]\n   [253]\n   [253]\n   [253]\n   [149]\n   [  7]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 26]\n   [221]\n   [253]\n   [253]\n   [253]\n   [253]\n   [254]\n   [253]\n   [253]\n   [253]\n   [253]\n   [149]\n   [  7]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  6]\n   [170]\n   [255]\n   [254]\n   [254]\n   [248]\n   [181]\n   [124]\n   [142]\n   [ 58]\n   [  7]\n   [124]\n   [255]\n   [109]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [123]\n   [253]\n   [238]\n   [ 95]\n   [ 18]\n   [ 17]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [169]\n   [223]\n   [ 21]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 19]\n   [226]\n   [240]\n   [ 58]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [118]\n   [253]\n   [ 36]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [108]\n   [251]\n   [119]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [112]\n   [253]\n   [ 36]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [128]\n   [235]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 33]\n   [228]\n   [253]\n   [ 36]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [128]\n   [235]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 64]\n   [228]\n   [255]\n   [140]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [128]\n   [251]\n   [138]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 51]\n   [145]\n   [249]\n   [253]\n   [192]\n   [ 14]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [109]\n   [245]\n   [253]\n   [192]\n   [108]\n   [ 37]\n   [ 43]\n   [128]\n   [186]\n   [250]\n   [253]\n   [250]\n   [151]\n   [ 13]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [114]\n   [215]\n   [254]\n   [253]\n   [253]\n   [253]\n   [253]\n   [254]\n   [253]\n   [159]\n   [ 69]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [ 59]\n   [189]\n   [201]\n   [240]\n   [253]\n   [253]\n   [163]\n   [ 78]\n   [ 11]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]\n\n  [[  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]\n   [  0]]]], shape=(1, 28, 28, 1), dtype=uint8),\n    tf.Tensor([8], shape=(1,), dtype=int64))\n  input_signature: (\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, data) in enumerate(train_mnist):\n",
    "    print(batch)\n",
    "    print(\"Input\")\n",
    "    print(data[\"image\"])\n",
    "    print(\"Target\")\n",
    "    print(data[\"label\"])\n",
    "    train_step(data[\"image\"], data[\"label\"])\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T04:16:37.772962Z",
     "iopub.status.busy": "2020-11-02T04:16:37.772298Z",
     "iopub.status.idle": "2020-11-02T04:16:37.774529Z",
     "shell.execute_reply": "2020-11-02T04:16:37.774013Z"
    },
    "id": "CN-BV43FMBej"
   },
   "outputs": [],
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T04:16:37.779773Z",
     "iopub.status.busy": "2020-11-02T04:16:37.779103Z",
     "iopub.status.idle": "2020-11-02T04:16:37.781360Z",
     "shell.execute_reply": "2020-11-02T04:16:37.780888Z"
    },
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T04:16:37.785148Z",
     "iopub.status.busy": "2020-11-02T04:16:37.784487Z",
     "iopub.status.idle": "2020-11-02T04:16:39.025891Z",
     "shell.execute_reply": "2020-11-02T04:16:39.025281Z"
    },
    "id": "YsxrAlvFG8SZ"
   },
   "outputs": [],
   "source": [
    "translate(\"este  um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T04:16:39.030035Z",
     "iopub.status.busy": "2020-11-02T04:16:39.029378Z",
     "iopub.status.idle": "2020-11-02T04:16:39.764247Z",
     "shell.execute_reply": "2020-11-02T04:16:39.763740Z"
    },
    "id": "7EH5y_aqI4t1"
   },
   "outputs": [],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T04:16:39.769311Z",
     "iopub.status.busy": "2020-11-02T04:16:39.768606Z",
     "iopub.status.idle": "2020-11-02T04:16:42.049202Z",
     "shell.execute_reply": "2020-11-02T04:16:42.049631Z"
    },
    "id": "J-hVCTSUMlkb"
   },
   "outputs": [],
   "source": [
    "translate(\"vou ento muito rapidamente partilhar convosco algumas histrias de algumas coisas mgicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1MxkSZvz0jX"
   },
   "source": [
    "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-02T04:16:42.054286Z",
     "iopub.status.busy": "2020-11-02T04:16:42.053550Z",
     "iopub.status.idle": "2020-11-02T04:16:43.727809Z",
     "shell.execute_reply": "2020-11-02T04:16:43.728233Z"
    },
    "id": "t-kFyiOLH0xg"
   },
   "outputs": [],
   "source": [
    "translate(\"este  o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqQ1fIsLwkGE"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.\n",
    "\n",
    "Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models. Futhermore, you can implement beam search to get better predictions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s_qNSzzyaCbD"
   ],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}