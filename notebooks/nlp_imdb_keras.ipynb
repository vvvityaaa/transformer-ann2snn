{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras IMDB sentiment analysis tutorial https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), \n",
    "                                        tf.keras.layers.Dense(embed_dim),])\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        ffn_output = self.ffn(attn_output)\n",
    "        return attn_output + ffn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen)\n",
    "\n",
    "# y_train = to_categorical(y_train, 2)\n",
    "# y_val = to_categorical(y_val, 2)\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# path_wd = os.path.join('/home/viktor/PycharmProjects/guided_research/snn_toolbox', '..', 'temp', str(time.time()))\n",
    "# os.makedirs(path_wd)\n",
    "# # Save dataset so SNN toolbox can find it.\n",
    "# np.savez_compressed(os.path.join(path_wd, 'x_test'), x_val)\n",
    "# np.savez_compressed(os.path.join(path_wd, 'y_test'), y_val)\n",
    "# # SNN toolbox will not do any training, but we save a subset of the training\n",
    "# # set so the toolbox can use it when normalizing the network parameters.\n",
    "# np.savez_compressed(os.path.join(path_wd, 'x_norm'), x_train[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"token_and_position_embedding_8/embedding_16/embedding_lookup/Identity_1:0\", shape=(None, 32), dtype=float32)\n",
      "Tensor(\"token_and_position_embedding_8/embedding_17/embedding_lookup/Identity_1:0\", shape=(None, 200, 32), dtype=float32)\n",
      "Epoch 1/2\n",
      "Tensor(\"model_8/token_and_position_embedding_8/embedding_16/embedding_lookup/Identity_1:0\", shape=(None, 32), dtype=float32)\n",
      "Tensor(\"model_8/token_and_position_embedding_8/embedding_17/embedding_lookup/Identity_1:0\", shape=(None, 200, 32), dtype=float32)\n",
      "Tensor(\"model_8/token_and_position_embedding_8/embedding_16/embedding_lookup/Identity_1:0\", shape=(None, 32), dtype=float32)\n",
      "Tensor(\"model_8/token_and_position_embedding_8/embedding_17/embedding_lookup/Identity_1:0\", shape=(None, 200, 32), dtype=float32)\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4845 - accuracy: 0.7340Tensor(\"model_8/token_and_position_embedding_8/embedding_16/embedding_lookup/Identity_1:0\", shape=(None, 32), dtype=float32)\n",
      "Tensor(\"model_8/token_and_position_embedding_8/embedding_17/embedding_lookup/Identity_1:0\", shape=(None, 200, 32), dtype=float32)\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.4844 - accuracy: 0.7341 - val_loss: 0.2929 - val_accuracy: 0.8784\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.1897 - accuracy: 0.9279 - val_loss: 0.3336 - val_accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model_8/token_and_position_embedding_8/embedding_16/embedding_lookup/Identity_1:0\", shape=(None, 32), dtype=float32)\n",
      "Tensor(\"model_8/token_and_position_embedding_8/embedding_17/embedding_lookup/Identity_1:0\", shape=(None, 200, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "l = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18443632, 0.81556374], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaleLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        scale = inputs[1]\n",
    "        return inputs[0] / scale\n",
    "\n",
    "\n",
    "class MatMulLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1])\n",
    "\n",
    "\n",
    "class MatMulLayerTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayerTranspose, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1], transpose_b=True)/np.sqrt(inputs[0].shape[-1])\n",
    "\n",
    "\n",
    "class StandardAdditionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(StandardAdditionLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.add(inputs[0], inputs[1])\n",
    "    \n",
    "    \n",
    "class CreateRangeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CreateRangeLayer, self).__init__()\n",
    "\n",
    "    def call(self, maxlen):\n",
    "        return tf.range(start=0, limit=maxlen, delta=1)\n",
    "    \n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.maxlen = 200\n",
    "        self.vocab_size = 20000\n",
    "        self.embed_dim = 32\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim)\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1233)\n",
    "inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "# positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "# positions = CreateRangeLayer()(maxlen)\n",
    "# positions = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)(positions)\n",
    "# x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "# positions = tf.broadcast_to(positions, (50, 200, 32))\n",
    "# positions = tf.keras.layers.Reshape([-1, 200, 32])(positions)\n",
    "# print(positions.shape)\n",
    "# print(x.shape)\n",
    "# x = x + positions\n",
    "# x = StandardAdditionLayer()([x, positions])\n",
    "# positions = tf.expand_dims(positions, axis=0)\n",
    "# print(positions.shape)\n",
    "# x = tf.keras.layers.Add()([x, positions])\n",
    "x = TokenAndPositionEmbedding()(inputs)\n",
    "# x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "# -------------- TRANSFORMER BLOCK -----------------\n",
    "\n",
    "dv = 25\n",
    "nv = -1\n",
    "\n",
    "v2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "q2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "k2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "\n",
    "v = tf.keras.layers.Reshape([embed_dim, nv, dv])(v2)\n",
    "q = tf.keras.layers.Reshape([embed_dim, nv, dv])(q2)\n",
    "k = tf.keras.layers.Reshape([embed_dim, nv, dv])(k2)\n",
    "\n",
    "# softmax(q*k^T/sqrt(dv))\n",
    "att = MatMulLayerTranspose()([q, k])\n",
    "att = tf.keras.layers.Softmax(axis=-1)(att)\n",
    "# softmax(q*k^T/sqrt(dv))*v\n",
    "out = MatMulLayer()([att, v])\n",
    "\n",
    "out = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(out)\n",
    "x = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(x)\n",
    "add = tf.keras.layers.Add()([out, x])\n",
    "# add = tf.add(out, x)\n",
    "\n",
    "x = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(add)\n",
    "x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "# --------------------------------------------------\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "# x = tf.keras.layers.Reshape([1, embed_dim, embed_dim])(x)\n",
    "x = tf.keras.layers.AveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged, input_shape = [200], output_shape = [200, 25]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-3a17fca82bbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# x = Tokposangles()(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0;31m# Set the static shape for the result since it might lost during array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m       \u001b[0;31m# reshape, eg, some `None` dim in the result could be inferred.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m       \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m       output_shape += self._fix_unknown_dimension(input_shape[1:],\n\u001b[0m\u001b[1;32m    548\u001b[0m                                                   self.target_shape)\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    534\u001b[0m       \u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged, input_shape = [200], output_shape = [200, 25]"
     ]
    }
   ],
   "source": [
    "class Tokposangles(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Tokposangles, self).__init__()\n",
    "        # TODO: remove fixed parametrization\n",
    "        self.maxlen = 200\n",
    "        self.vocab_size = 20000\n",
    "        self.embed_dim = 32\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, name=\"positional\")\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, name=\"token\")\n",
    "        self.positions = self.positional_encoding(self.maxlen, self.embed_dim)\n",
    "        \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "#         x = tf.reshape(x, [-1, maxlen, self.embed_dim])\n",
    "        x = self.token_emb(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        return x + self.positions[:, :seq_len, :]\n",
    "    \n",
    "tf.random.set_seed(123)\n",
    "inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "# x = Tokposangles()(inputs)\n",
    "pos = positional_encoding(maxlen, 25)\n",
    "x = tf.keras.layers.Reshape([200, 25])(inputs)\n",
    "import pdb;pdb.set_trace()\n",
    "x = x + pos\n",
    "# -------------- TRANSFORMER BLOCK -----------------\n",
    "\n",
    "dv = 25\n",
    "nv = -1\n",
    "\n",
    "v2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "q2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "k2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "\n",
    "v = tf.keras.layers.Reshape([embed_dim, nv, dv])(v2)\n",
    "q = tf.keras.layers.Reshape([embed_dim, nv, dv])(q2)\n",
    "k = tf.keras.layers.Reshape([embed_dim, nv, dv])(k2)\n",
    "\n",
    "# softmax(q*k^T/sqrt(dv))\n",
    "att = MatMulLayerTranspose()([q, k])\n",
    "att = tf.keras.layers.Softmax(axis=-1)(att)\n",
    "# softmax(q*k^T/sqrt(dv))*v\n",
    "out = MatMulLayer()([att, v])\n",
    "\n",
    "out = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(out)\n",
    "x = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(x)\n",
    "add = tf.keras.layers.Add()([out, x])\n",
    "\n",
    "x = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(add)\n",
    "x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "# --------------------------------------------------\n",
    "x = tf.keras.layers.AveragePooling2D()(x)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 20s 25ms/step - loss: 0.3088 - accuracy: 0.8676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30878788232803345, 0.8675600290298462]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding_1  (None, 200, 32)      646400      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200, 32)      1056        token_and_position_embedding_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200, 32)      1056        token_and_position_embedding_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 8, 25)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 32, 8, 25)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_transpose (MatMul (None, 32, 8, 8)     0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200, 32)      1056        token_and_position_embedding_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 32, 8, 8)     0           mat_mul_layer_transpose[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 8, 25)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer (MatMulLayer)     (None, 32, 8, 25)    0           softmax[0][0]                    \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 32, 200, 1)   0           mat_mul_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 32, 200, 1)   0           token_and_position_embedding_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 200, 1)   0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32, 200, 32)  64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32, 200, 32)  1056        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 100, 32)  0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16, 100, 32)  0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 51200)        0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           1024020     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            42          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,674,750\n",
      "Trainable params: 1,674,750\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "keras.models.save_model(\n",
    "    model, \n",
    "    os.path.join(\"/home/viktor/PycharmProjects/guided_research/transformer-to-snn-conversion\", \n",
    "    \"mnist_transformer\" + '.h5')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\n",
    "    os.path.join(\"/home/viktor/PycharmProjects/guided_research/transformer-to-snn-conversion\", \n",
    "    \"mnist_transformer\" + '.h5'), custom_objects={'MatMulLayer': MatMulLayer, \n",
    "                                                  'MatMulLayerTranspose': MatMulLayerTranspose,\n",
    "#                                                   'CreateRangeLayer': CreateRangeLayer,\n",
    "#                                                   'StandardAdditionLayer': StandardAdditionLayer,\n",
    "                                                  'TokenAndPositionEmbedding': TokenAndPositionEmbedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Load the data, keeping only 10,000 of the most frequently occuring words\n",
    "(train_data, train_labels), (val_data, val_labels) = imdb.load_data(num_words = 5000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=5000):\n",
    "    results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1                        # Sets specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Vectorize training Data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Vectorize testing Data\n",
    "x_val = vectorize_sequences(val_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_val  = np.asarray(val_labels).astype('float32')\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_val = to_categorical(y_val, 2)\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# path_wd = os.path.join('/home/viktor/PycharmProjects/guided_research/snn_toolbox', '..', 'temp', str(time.time()))\n",
    "# os.makedirs(path_wd)\n",
    "# # Save dataset so SNN toolbox can find it.\n",
    "# np.savez_compressed(os.path.join(path_wd, 'x_test'), x_val)\n",
    "# np.savez_compressed(os.path.join(path_wd, 'y_test'), y_val)\n",
    "# # SNN toolbox will not do any training, but we save a subset of the training\n",
    "# # set so the toolbox can use it when normalizing the network parameters.\n",
    "# np.savez_compressed(os.path.join(path_wd, 'x_norm'), x_train[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokposangles(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Tokposangles, self).__init__()\n",
    "        # TODO: remove fixed parametrization\n",
    "        self.maxlen = 200\n",
    "        self.vocab_size = 20000\n",
    "        self.embed_dim = 32\n",
    "        # self.pos_emb = tf.keras.layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, name=\"positional\")\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, name=\"token\")\n",
    "        self.positions = self.positional_encoding(self.maxlen, self.embed_dim)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.token_emb(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        return x + self.positions[:, :seq_len, :]\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=20000)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, 200)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, 200)\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_val = to_categorical(y_val, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(200,))\n",
    "# x = tf.keras.layers.Embedding(input_dim=20000, output_dim=32, name=\"token\")(inputs)\n",
    "# pos = positional_encoding(200, 25)\n",
    "# x = tf.keras.layers.Reshape([200, 25])(inputs)\n",
    "# x = x + pos\n",
    "x = tf.keras.layers.Dense(200, activation='relu')(inputs)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# x = tf.keras.layers.Reshape([4, 2, 8])(x)\n",
    "# x = tf.expand_dims(x, axis=1)\n",
    "# x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "# print(x.shape)\n",
    "# x = tf.squeeze(x, axis=1)\n",
    "x = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "196/196 [==============================] - 1s 2ms/step - loss: 299.5150 - accuracy: 0.5000 - val_loss: 44.2433 - val_accuracy: 0.4997\n",
      "Epoch 2/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 26.3840 - accuracy: 0.5399 - val_loss: 8.9008 - val_accuracy: 0.5043\n",
      "Epoch 3/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 6.0700 - accuracy: 0.5413 - val_loss: 4.0968 - val_accuracy: 0.5005\n",
      "Epoch 4/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 3.1542 - accuracy: 0.5461 - val_loss: 3.0592 - val_accuracy: 0.5005\n",
      "Epoch 5/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 2.1334 - accuracy: 0.5471 - val_loss: 4.0115 - val_accuracy: 0.5017\n",
      "Epoch 6/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 1.9434 - accuracy: 0.5472 - val_loss: 3.1104 - val_accuracy: 0.5015\n",
      "Epoch 7/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 1.8843 - accuracy: 0.5425 - val_loss: 2.1375 - val_accuracy: 0.5034\n",
      "Epoch 8/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 1.3355 - accuracy: 0.5425 - val_loss: 3.0222 - val_accuracy: 0.5007\n",
      "Epoch 9/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 1.3874 - accuracy: 0.5424 - val_loss: 1.8267 - val_accuracy: 0.5040\n",
      "Epoch 10/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.9043 - accuracy: 0.5606 - val_loss: 1.7632 - val_accuracy: 0.5024\n",
      "Epoch 11/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.9017 - accuracy: 0.5490 - val_loss: 1.8376 - val_accuracy: 0.5017\n",
      "Epoch 12/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.8908 - accuracy: 0.5520 - val_loss: 2.1916 - val_accuracy: 0.5024\n",
      "Epoch 13/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.8164 - accuracy: 0.5517 - val_loss: 1.5831 - val_accuracy: 0.5001\n",
      "Epoch 14/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.7625 - accuracy: 0.5527 - val_loss: 1.4816 - val_accuracy: 0.5020\n",
      "Epoch 15/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.7763 - accuracy: 0.5573 - val_loss: 1.4483 - val_accuracy: 0.5018\n",
      "Epoch 16/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.7403 - accuracy: 0.5438 - val_loss: 1.4105 - val_accuracy: 0.5039\n",
      "Epoch 17/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.7603 - accuracy: 0.5380 - val_loss: 1.3306 - val_accuracy: 0.5008\n",
      "Epoch 18/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.7304 - accuracy: 0.5375 - val_loss: 1.2687 - val_accuracy: 0.5030\n",
      "Epoch 19/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.7223 - accuracy: 0.5380 - val_loss: 1.3574 - val_accuracy: 0.5020\n",
      "Epoch 20/20\n",
      "196/196 [==============================] - 0s 1ms/step - loss: 0.7170 - accuracy: 0.5381 - val_loss: 1.1078 - val_accuracy: 0.5075\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                   y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=128,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 200, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 200\n",
    "d = 16\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0]\n",
    "\n",
    "# Juggle the dimensions for the plot\n",
    "pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n",
    "pos_encoding = tf.transpose(pos_encoding, (2,1,0))\n",
    "pos_encoding = tf.reshape(pos_encoding, (d, n))\n",
    "\n",
    "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                80016     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "tf.reshape_1 (TFOpLambda)    (None, 4, 2, 8)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 81,122\n",
      "Trainable params: 81,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "\n",
    "# path to the directory where the original dataset was uncompressed\n",
    "imdb_dir = 'aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "  dir_name = os.path.join(train_dir, label_type)\n",
    "\n",
    "  for fname in os.listdir(dir_name):\n",
    "    if fname[-4:] == '.txt':\n",
    "      f = open(os.path.join(dir_name, fname))\n",
    "      texts.append(f.read())\n",
    "      f.close()\n",
    "\n",
    "      if label_type == 'neg':\n",
    "          labels.append(0)\n",
    "      else:\n",
    "          labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 200)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "maxlen = 200 # cuts off reviews after 100 words\n",
    "training_samples = 20000 # trains on 200 samples\n",
    "validation_samples = 5000\n",
    "max_words = 10000 # considers only the top 10,000 words in the dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_val = to_categorical(y_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.6B.100d.txt')\n",
    "\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "  if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.038194  , -0.24487001,  0.72812003, ..., -0.1459    ,\n",
       "         0.82779998,  0.27061999],\n",
       "       [-0.071953  ,  0.23127   ,  0.023731  , ..., -0.71894997,\n",
       "         0.86894   ,  0.19539   ],\n",
       "       ...,\n",
       "       [-0.25373   , -0.88200998, -0.068905  , ..., -0.092328  ,\n",
       "         0.026667  , -0.17737   ],\n",
       "       [-0.7888    ,  0.97996002, -0.019584  , ..., -0.047364  ,\n",
       "         0.51482999, -0.39811999],\n",
       "       [ 0.40972999,  0.39761999,  0.75586998, ...,  0.21242   ,\n",
       "         0.25915   ,  0.87817001]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(max_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_words,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88582"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 4s 25ms/step - loss: 2.6346 - acc: 0.4872 - val_loss: 1.2442 - val_acc: 0.4992\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 4s 25ms/step - loss: 0.7517 - acc: 0.6528 - val_loss: 1.2002 - val_acc: 0.5086\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 4s 25ms/step - loss: 0.5805 - acc: 0.7223 - val_loss: 1.3185 - val_acc: 0.4986\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 4s 25ms/step - loss: 0.5056 - acc: 0.7572 - val_loss: 1.2357 - val_acc: 0.4890\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 4s 25ms/step - loss: 0.4497 - acc: 0.7855 - val_loss: 1.2676 - val_acc: 0.4980\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 4s 25ms/step - loss: 0.4028 - acc: 0.8153 - val_loss: 1.3342 - val_acc: 0.4954\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 4s 26ms/step - loss: 0.4148 - acc: 0.8052 - val_loss: 1.3554 - val_acc: 0.4974\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 4s 27ms/step - loss: 0.3268 - acc: 0.8587 - val_loss: 1.4659 - val_acc: 0.4888\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 4s 27ms/step - loss: 0.3034 - acc: 0.8715 - val_loss: 1.6597 - val_acc: 0.4924\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 4s 26ms/step - loss: 0.3079 - acc: 0.8672 - val_loss: 1.6401 - val_acc: 0.4926\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Flatten, Dense, Input\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "\n",
    "embedding_layer = Embedding(max_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_words,\n",
    "                            trainable=False)(inputs)\n",
    "# x = Embedding(input_dim=max_words, output_dim=embedding_dim, trainable=False)(inputs)\n",
    "x = Flatten()(embedding_layer)\n",
    "x = Dense(250, activation='relu')(x)\n",
    "outputs = Dense(10, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "               metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-snn-conversion",
   "language": "python",
   "name": "transformer-snn-conversion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
