{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras IMDB sentiment analysis tutorial https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([tf.keras.layers.Dense(ff_dim, activation=\"relu\"), \n",
    "                                        tf.keras.layers.Dense(embed_dim),])\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        ffn_output = self.ffn(attn_output)\n",
    "        return attn_output + ffn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/viktor/.local/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/.local/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_val = to_categorical(y_val, 2)\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# path_wd = os.path.join('/home/viktor/PycharmProjects/guided_research/snn_toolbox', '..', 'temp', str(time.time()))\n",
    "# os.makedirs(path_wd)\n",
    "# # Save dataset so SNN toolbox can find it.\n",
    "# np.savez_compressed(os.path.join(path_wd, 'x_test'), x_val)\n",
    "# np.savez_compressed(os.path.join(path_wd, 'y_test'), y_val)\n",
    "# # SNN toolbox will not do any training, but we save a subset of the training\n",
    "# # set so the toolbox can use it when normalizing the network parameters.\n",
    "# np.savez_compressed(os.path.join(path_wd, 'x_norm'), x_train[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScaleLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        scale = inputs[1]\n",
    "        return inputs[0] / scale\n",
    "\n",
    "\n",
    "class MatMulLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1])\n",
    "\n",
    "\n",
    "class MatMulLayerTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatMulLayerTranspose, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], inputs[1], transpose_b=True)/np.sqrt(inputs[0].shape[-1])\n",
    "\n",
    "\n",
    "class StandardAdditionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(StandardAdditionLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.add(inputs[0], inputs[1])\n",
    "    \n",
    "    \n",
    "class CreateRangeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CreateRangeLayer, self).__init__()\n",
    "\n",
    "    def call(self, maxlen):\n",
    "        return tf.range(start=0, limit=maxlen, delta=1)\n",
    "    \n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.maxlen = 200\n",
    "        self.vocab_size = 20000\n",
    "        self.embed_dim = 32\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim)\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32, 200, 32)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(maxlen,))\n",
    "# positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "# positions = CreateRangeLayer()(maxlen)\n",
    "# positions = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)(positions)\n",
    "# x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "# positions = tf.broadcast_to(positions, (50, 200, 32))\n",
    "# positions = tf.keras.layers.Reshape([-1, 200, 32])(positions)\n",
    "# print(positions.shape)\n",
    "# print(x.shape)\n",
    "# x = x + positions\n",
    "# x = StandardAdditionLayer()([x, positions])\n",
    "# positions = tf.expand_dims(positions, axis=0)\n",
    "# print(positions.shape)\n",
    "# x = tf.keras.layers.Add()([x, positions])\n",
    "\n",
    "x = TokenAndPositionEmbedding()(inputs)\n",
    "# x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "# -------------- TRANSFORMER BLOCK -----------------\n",
    "\n",
    "dv = 25\n",
    "nv = -1\n",
    "\n",
    "v2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "q2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "k2 = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(x)\n",
    "\n",
    "v = tf.keras.layers.Reshape([embed_dim, nv, dv])(v2)\n",
    "q = tf.keras.layers.Reshape([embed_dim, nv, dv])(q2)\n",
    "k = tf.keras.layers.Reshape([embed_dim, nv, dv])(k2)\n",
    "\n",
    "# softmax(q*k^T/sqrt(dv))\n",
    "att = MatMulLayerTranspose()([q, k])\n",
    "# att = ScaleLayer()([att, np.sqrt(dv)])\n",
    "att = tf.keras.layers.Softmax(axis=-1)(att)\n",
    "# softmax(q*k^T/sqrt(dv))*v\n",
    "out = MatMulLayer()([att, v])\n",
    "\n",
    "out = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(out)\n",
    "x = tf.keras.layers.Reshape([embed_dim, maxlen, 1])(x)\n",
    "add = tf.keras.layers.Add()([out, x])\n",
    "# add = tf.add(out, x)\n",
    "\n",
    "x = tf.keras.layers.Dense(embed_dim, activation=\"relu\")(add)\n",
    "x = tf.keras.layers.Dense(embed_dim)(x)\n",
    "# --------------------------------------------------\n",
    "# x = tf.keras.layers.Flatten()(x)\n",
    "# x = tf.keras.layers.Reshape([1, embed_dim, embed_dim])(x)\n",
    "print(x.shape)\n",
    "x = tf.keras.layers.AveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 70s 178ms/step - loss: 0.6021 - accuracy: 0.6174 - val_loss: 0.4041 - val_accuracy: 0.8187\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 22s 28ms/step - loss: 0.3027 - accuracy: 0.8704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30267125368118286, 0.8704400062561035]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09912069, -0.19579193, -0.16229156, ...,  0.10588318,\n",
       "         0.25742522,  0.1727877 ],\n",
       "       [-0.22689337,  0.25866896,  0.00569219, ...,  0.02986109,\n",
       "         0.03099819,  0.00711283],\n",
       "       [ 0.26708716,  0.0849209 , -0.09715092, ...,  0.16539818,\n",
       "        -0.21219455,  0.29617092],\n",
       "       ...,\n",
       "       [ 0.01515449, -0.19789135, -0.06235372, ...,  0.14110392,\n",
       "         0.29501003,  0.06025026],\n",
       "       [-0.02646763, -0.04001221,  0.06900238, ...,  0.04967032,\n",
       "         0.01653934,  0.00669733],\n",
       "       [-0.10428838,  0.30174482, -0.24962968, ..., -0.05366444,\n",
       "         0.2063334 , -0.25843057]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_26 (Embedding)        (None, 200, 32)      640000      input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, 200, 32)      1056        embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, 200, 32)      1056        embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_92 (Reshape)            (None, 32, 8, 25)    0           dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_93 (Reshape)            (None, 32, 8, 25)    0           dense_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_transpose_20 (Mat (None, 32, 8, 8)     0           reshape_92[0][0]                 \n",
      "                                                                 reshape_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_115 (Dense)               (None, 200, 32)      1056        embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "softmax_18 (Softmax)            (None, 32, 8, 8)     0           mat_mul_layer_transpose_20[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_91 (Reshape)            (None, 32, 8, 25)    0           dense_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mat_mul_layer_20 (MatMulLayer)  (None, 32, 8, 25)    0           softmax_18[0][0]                 \n",
      "                                                                 reshape_91[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_94 (Reshape)            (None, 32, 200, 1)   0           mat_mul_layer_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_95 (Reshape)            (None, 32, 200, 1)   0           embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 32, 200, 1)   0           reshape_94[0][0]                 \n",
      "                                                                 reshape_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, 32, 200, 32)  64          add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_119 (Dense)               (None, 32, 200, 32)  1056        dense_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 100, 32)  0           dense_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 100, 32)  0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 51200)        0           dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_120 (Dense)               (None, 20)           1024020     flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 20)           0           dense_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_121 (Dense)               (None, 2)            42          dropout_21[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,668,350\n",
      "Trainable params: 1,668,350\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "keras.models.save_model(\n",
    "    model, \n",
    "    os.path.join(\"/home/viktor/PycharmProjects/guided_research/transformer-to-snn-conversion\", \n",
    "    \"mnist_transformer\" + '.h5')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\n",
    "    os.path.join(\"/home/viktor/PycharmProjects/guided_research/transformer-to-snn-conversion\", \n",
    "    \"mnist_transformer\" + '.h5'), custom_objects={'MatMulLayer': MatMulLayer, \n",
    "                                                  'MatMulLayerTranspose': MatMulLayerTranspose,\n",
    "#                                                   'CreateRangeLayer': CreateRangeLayer,\n",
    "#                                                   'StandardAdditionLayer': StandardAdditionLayer,\n",
    "                                                  'TokenAndPositionEmbedding': TokenAndPositionEmbedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-snn-conversion",
   "language": "python",
   "name": "transformer-snn-conversion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}