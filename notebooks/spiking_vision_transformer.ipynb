{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "\"\"\"\n",
    "All credits to Daniel A. Code is used from a tutorial\n",
    "\"\"\"\n",
    "\n",
    "# ====================== SPIKING PART =================================\n",
    "@tf.custom_gradient\n",
    "def spike_function(v_to_threshold: tf.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    A custom gradient for networks of spiking neurons.\n",
    "\n",
    "    @param v_to_threshold: The difference between current and threshold voltage of the neuron.\n",
    "    @type v_to_threshold: tf.float32\n",
    "    @return: Activation z and gradient grad.\n",
    "    @rtype: tuple\n",
    "    \"\"\"\n",
    "    z = tf.cast(tf.greater(v_to_threshold, 1.), dtype=tf.float32)\n",
    "\n",
    "    def grad(dy: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        The gradient function for calculating the derivative of the spike-function.\n",
    "\n",
    "        The return value is determined as follows:\n",
    "\n",
    "        # @negative: v_to_threshold < 0 -> dy*0\n",
    "        # @rest: v_to_threshold = 0 -> dy*0+\n",
    "        # @thresh: v_to_threshold = 1 -> dy*1\n",
    "        # @+thresh: v_to_threshold > 1 -> dy*1-\n",
    "        # @2thresh: v_to_threshold > 2 -> dy*0\n",
    "        #\n",
    "        #         /\\\n",
    "        #        /  \\\n",
    "        # ______/    \\______\n",
    "        # -1   0   1  2   3  v_to_threshold\n",
    "\n",
    "        @param dy: The previous upstream gradient.\n",
    "        @return: The calculated gradient of this stage of the network\n",
    "        \"\"\"\n",
    "        return [dy * tf.maximum(1 - tf.abs(v_to_threshold - 1), 0)]\n",
    "    return z, grad\n",
    "\n",
    "class IntegratorNeuronCell(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A simple spiking neuron layer that integrates (sums up) the outputs of the previous layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_neurons, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialization function of the IntegratorNeuronCell.\n",
    "\n",
    "        @param n_in: Number of inputs, i.e. outputs of previous layer.\n",
    "        @param n_neurons: Number of neurons, i.e. outputs of this layer.\n",
    "        @param kwargs: Additional parameters, forwarded to standard Layer init function of tf.\n",
    "        \"\"\"\n",
    "        super(IntegratorNeuronCell, self).__init__(**kwargs)\n",
    "        self.n_in = n_in\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.w_in = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Creates the variables of this layer, i.e. creates and initializes the weights\n",
    "        for all neurons within this layer.\n",
    "\n",
    "        @param input_shape: Not needed for this layer.\n",
    "        @type input_shape:\n",
    "        \"\"\"\n",
    "        del input_shape  # Unused\n",
    "\n",
    "        w_in = tf.random.normal((self.n_in, self.n_neurons), dtype=self.dtype)\n",
    "        self.w_in = tf.Variable(initial_value=w_in / np.sqrt(self.n_in), trainable=True)\n",
    "\n",
    "    @property\n",
    "    def state_size(self) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Returns the state size depicted of cell and hidden state  as a tuple of number of neurons, number of neurons.\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        return self.n_neurons, self.n_neurons\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        \"\"\"\n",
    "\n",
    "        @param inputs:\n",
    "        @param batch_size:\n",
    "        @param dtype:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        del inputs  # Unused\n",
    "\n",
    "        zeros = tf.zeros((batch_size, self.n_neurons), dtype=dtype)\n",
    "        return zeros, zeros\n",
    "\n",
    "    def call(self, input_at_t, states_at_t):\n",
    "        \"\"\"\n",
    "\n",
    "        @param input_at_t:\n",
    "        @param states_at_t:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        old_v, old_z = states_at_t\n",
    "\n",
    "        i_t = tf.matmul(input_at_t, self.w_in)\n",
    "\n",
    "        new_v = old_v + i_t\n",
    "        new_z = tf.nn.softmax(new_v)\n",
    "\n",
    "        return new_z, (new_v, new_z)\n",
    "\n",
    "class LifNeuronCell(IntegratorNeuronCell):\n",
    "    \"\"\"\n",
    "    A more advanced spiking tf layer building upon the IntegratorNeuronCell,\n",
    "    but augmenting it with a leaky and fire functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in: int, n_neurons: int, tau: float = 20., threshold: float = 0.1,\n",
    "                 activation_function: Callable[[tf.Tensor], tuple] = spike_function, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes a (Recurrent)LifNeuronCell.\n",
    "\n",
    "        @param n_in: Number of inputs, i.e. outputs of previous layer.\n",
    "        @param n_neurons: Number of neurons, i.e. outputs of this layer.\n",
    "        @param tau: The time constant tau.\n",
    "        @param threshold: The threshold for the neurons in this layer.\n",
    "        @param activation_function: The activation function for the LIF-Neuron, defaults to a simple spike-function.\n",
    "        @param kwargs: Additional parameters, forwarded to standard Layer init function of tf.\n",
    "        \"\"\"\n",
    "        super(LifNeuronCell, self).__init__(n_in, n_neurons, **kwargs)\n",
    "        self.tau = tau\n",
    "        self.decay = tf.exp(-1/tau)\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def call(self, input_at_t, states_at_t):\n",
    "        old_v, old_z = states_at_t\n",
    "\n",
    "        i_t = tf.matmul(input_at_t, self.w_in)\n",
    "        i_reset = old_z * self.threshold\n",
    "\n",
    "        new_v = self.decay * old_v + (1.0 - self.decay) * i_t - i_reset\n",
    "        new_z = self.activation_function(new_v/self.threshold)\n",
    "\n",
    "        return new_z, (new_v, new_z)\n",
    "\n",
    "\n",
    "class RecurrentLifNeuronCell(LifNeuronCell):\n",
    "    \"\"\"\n",
    "    A recurrent spiking layer implementing a recurrent layer of LIF-Neurons.\n",
    "    Each neuron has a connection to the previous/next layer as well recurrent\n",
    "    connection to itself.\n",
    "    \"\"\"\n",
    "    def build(self, input_shape):\n",
    "        del input_shape  # Unused\n",
    "\n",
    "        w_in = tf.random.normal((self.n_in, self.n_neurons), dtype=self.dtype)\n",
    "        self.w_in = tf.Variable(initial_value=w_in / np.sqrt(self.n_in), trainable=True)\n",
    "\n",
    "        w_rec = tf.random.normal((self.n_neurons, self.n_neurons), dtype=self.dtype)\n",
    "        w_rec = tf.linalg.set_diag(w_rec, np.zeros(self.n_neurons))\n",
    "        self.w_rec = tf.Variable(initial_value=w_rec / np.sqrt(self.n_neurons), trainable=True)\n",
    "\n",
    "    def call(self, input_at_t, states_at_t):\n",
    "        old_v, old_z = states_at_t\n",
    "\n",
    "        i_t = tf.matmul(input_at_t, self.w_in) + tf.matmul(old_z, self.w_rec)\n",
    "        i_reset = old_z * self.threshold\n",
    "\n",
    "        new_v = self.decay * old_v + (1.0 - self.decay) * i_t - i_reset\n",
    "        new_z = self.activation_function(new_v/self.threshold)\n",
    "\n",
    "        return new_z, (new_v, new_z)\n",
    "\n",
    "# ================================= END OF SPIKING UTILITY PART ==================================\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = IntegratorNeuronCell(1, embed_dim)\n",
    "        self.key_dense = IntegratorNeuronCell(1, embed_dim)\n",
    "        self.value_dense = IntegratorNeuronCell(1, embed_dim)\n",
    "        self.combine_heads = IntegratorNeuronCell(1, embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.mlp = tf.keras.Sequential(\n",
    "            [\n",
    "                IntegratorNeuronCell(1, mlp_dim),\n",
    "                IntegratorNeuronCell(mlp_dim, embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        out1 = attn_output + inputs\n",
    "\n",
    "        mlp_output = self.mlp(out1)\n",
    "        return mlp_output + out1\n",
    "\n",
    "\n",
    "class VisionTransformer(tf.keras.Model):\n",
    "    def __init__(self, image_size, patch_size, num_layers, num_classes, d_model, num_heads, mlp_dim, channels=1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rescale = Rescaling(1.0 / 255)\n",
    "        self.pos_emb = self.add_weight(\"pos_emb\", shape=(1, num_patches + 1, d_model))\n",
    "        self.class_emb = self.add_weight(\"class_emb\", shape=(1, 1, d_model))\n",
    "        self.patch_proj = IntegratorNeuronCell(1, d_model)\n",
    "        self.enc_layers = [\n",
    "            TransformerBlock(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.mlp_head = tf.keras.Sequential(\n",
    "            [\n",
    "                IntegratorNeuronCell(1, mlp_dim),\n",
    "                IntegratorNeuronCell(1, num_classes),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def extract_patches(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\n",
    "        return patches\n",
    "\n",
    "    def call(self, x, training):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = self.rescale(x)\n",
    "        patches = self.extract_patches(x)\n",
    "        x = self.patch_proj(patches)\n",
    "\n",
    "        class_emb = tf.broadcast_to(self.class_emb, [batch_size, 1, self.d_model])\n",
    "        x = tf.concat([class_emb, x], axis=1)\n",
    "        x = x + self.pos_emb\n",
    "\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training)\n",
    "\n",
    "        # First (class token) is used for classification\n",
    "        x = self.mlp_head(x[:, 0])\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}