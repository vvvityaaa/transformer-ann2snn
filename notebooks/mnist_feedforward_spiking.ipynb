{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/viktor/PycharmProjects/guided_research/transformer-to-snn-conversion/spiking_tf-master\")\n",
    "from spiking_tf.src import spiking_models, plots, file_handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1\n",
    "max_rate = 2\n",
    "\n",
    "n_in = 28*28\n",
    "n_hidden = 800\n",
    "n_out = 10\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "thr = 0.1\n",
    "tau = 10.0\n",
    "\n",
    "output_path = file_handling.get_default_path_str()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPIKING NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer LifNeuronCell has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer LifNeuronCell has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/469 [..............................] - ETA: 24s - loss: 20.8594 - sparse_categorical_accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0143s vs `on_train_batch_end` time: 0.0905s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0143s vs `on_train_batch_end` time: 0.0905s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 2s 4ms/step - loss: 20.8879 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 20.8995 - val_sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "def spikalize_img(image, label):\n",
    "    '''Transform image to spikes. Spike with poisson distributed rate proportional to pixel brightness.'''\n",
    "    flattened = tf.reshape(image, [28*28])\n",
    "    rand = tf.random.uniform(shape=[timesteps, 28*28])\n",
    "    return tf.cast(flattened/255*max_rate > rand, tf.float32), label\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "(ds_train, ds_test) = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True\n",
    ")\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    spikalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(batch_size)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(\n",
    "    spikalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(timesteps, 28*28))\n",
    "mid_z = tf.keras.layers.Layer(spiking_models.LifNeuronCell(n_in, n_hidden, tau=tau, threshold=thr))(inputs)\n",
    "out_z = tf.keras.layers.Layer(spiking_models.IntegratorNeuronCell(n_hidden, n_out))(mid_z)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=[out_z])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train, epochs=epochs, validation_data=ds_test, callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 1ms/step - loss: 20.8995 - sparse_categorical_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20.899494171142578, 0.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEIGHT CONVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 784) for input Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32), but it was called on an input with incompatible shape (None, 1, 784).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 784) for input Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32), but it was called on an input with incompatible shape (None, 1, 784).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 784) for input Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32), but it was called on an input with incompatible shape (None, 1, 784).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 784) for input Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32), but it was called on an input with incompatible shape (None, 1, 784).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/469 [..............................] - ETA: 14s - loss: 145.8799 - sparse_categorical_accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/469 [============================>.] - ETA: 0s - loss: 5.1825 - sparse_categorical_accuracy: 0.9091WARNING:tensorflow:Model was constructed with shape (None, 784) for input Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32), but it was called on an input with incompatible shape (None, 1, 784).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 784) for input Tensor(\"dense_input:0\", shape=(None, 784), dtype=float32), but it was called on an input with incompatible shape (None, 1, 784).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "469/469 [==============================] - 2s 4ms/step - loss: 5.0715 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.7766 - val_sparse_categorical_accuracy: 0.9449\n"
     ]
    }
   ],
   "source": [
    "def flatten(image, label):\n",
    "    '''Transform image to the flattened version of itself'''\n",
    "    flattened = tf.reshape(image, [28*28])\n",
    "    flattened = tf.expand_dims(flattened, 0)\n",
    "    return tf.cast(flattened, tf.float32), label\n",
    "\n",
    "tf.random.set_seed(1234) \n",
    "\n",
    "(ds_train, ds_test) = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)\n",
    "ds_train = ds_train.map(flatten, num_parallel_calls=tf.data.experimental.AUTOTUNE).cache().shuffle(batch_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(flatten, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(800, activation='relu', input_shape=(784,), use_bias=False),\n",
    "  tf.keras.layers.Dense(10, use_bias=False),\n",
    "])\n",
    "\n",
    "# IMPORTANT > Model can be defined in both ways, you need to pass each layer in a different way\n",
    "\n",
    "# input_ = tf.keras.Input(shape=(784,))\n",
    "# middle = tf.keras.layers.Dense(800, activation='relu', use_bias=False)(input_)\n",
    "# out = tf.keras.layers.Dense(10, use_bias=False)(middle)\n",
    "# model = tf.keras.Model(input_, out)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "log_dir = \"logs/fit/analog_feedforward\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train, epochs=epochs, validation_data=ds_test, callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations_layer(layer_in, layer_out, x, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = 10\n",
    "\n",
    "    if len(x) % batch_size != 0:\n",
    "        x = x[: -(len(x) % batch_size)]\n",
    "\n",
    "    return Model(layer_in, layer_out).predict(x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_conversion_model(weights, bias):\n",
    "\n",
    "    # Get weights from trained network\n",
    "    converted_weights = weights\n",
    "    converted_bias = bias\n",
    "\n",
    "    # model based normalization\n",
    "    previous_factor = 1\n",
    "    for l in range(len(converted_weights)):\n",
    "        max_pos_input = 0\n",
    "        # Find maximum input for this layer\n",
    "        for o in range(converted_weights[l].shape[0]):\n",
    "            input_sum = 0\n",
    "            for i in range(converted_weights[l].shape[1]):\n",
    "                input_sum += tf.math.maximum(0, converted_weights[l][o, i])\n",
    "            if converted_bias is not None and converted_bias[l] is not None:\n",
    "                input_sum += tf.math.maximum(0, converted_bias[l][o])\n",
    "            max_pos_input = tf.math.maximum(max_pos_input, input_sum)\n",
    "\n",
    "        # get the maximum weight in the layer, in case all weights are negative, max_pos_input would be zero, so we use the max weight to rescale instead\n",
    "        max_wt = tf.math.reduce_max(converted_weights[l])\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            max_bias = tf.math.reduce_max(converted_bias[l])\n",
    "            max_wt = tf.math.maximum(max_wt, max_bias)\n",
    "        scale_factor = tf.math.maximum(max_wt, max_pos_input)\n",
    "        # Rescale all weights\n",
    "        applied_factor = scale_factor/previous_factor\n",
    "        converted_weights[l] = converted_weights[l] / applied_factor\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            converted_bias[l] = converted_bias[l] / scale_factor\n",
    "        previous_factor = scale_factor\n",
    "        print(f\"Scale factor for this layer is {previous_factor}\")\n",
    "            \n",
    "    return converted_weights, converted_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_conversion_robust_and_data_based(weights, bias, model, data, normalization_method='robust', ppercentile=0.99):\n",
    "\n",
    "    if normalization_method == 'data':\n",
    "        ppercentile = 1.0\n",
    "\n",
    "    # Get weights from trained network\n",
    "    converted_weights = weights\n",
    "    converted_bias = bias\n",
    "    \n",
    "    # use training set to find max_act for each neuron\n",
    "            \n",
    "    activations = []\n",
    "    for l in range(0, len(converted_weights)):\n",
    "        activation = get_activations_layer(model.input, model.layers[l].output, data)\n",
    "        activation_per_neuron = [np.max(activation[:, i]) for i in range(activation.shape[1])]\n",
    "        activations.append(activation_per_neuron)\n",
    "        \n",
    "    previous_factor = 1\n",
    "    for l in range(len(converted_weights)):\n",
    "        # get the p-percentile of the activation\n",
    "        pos_inputs = activations[l]\n",
    "        pos_inputs.sort()\n",
    "        max_act = pos_inputs[int(ppercentile * (len(pos_inputs) - 1))]\n",
    "        # get the maximum weight in the layer\n",
    "        max_wt = tf.math.reduce_max(converted_weights[l])\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            max_bias = tf.math.reduce_max(converted_bias[l])\n",
    "            max_wt = tf.math.maximum(max_wt, max_bias)\n",
    "        scale_factor = tf.math.maximum(max_wt, max_act)\n",
    "\n",
    "        applied_factor = scale_factor / previous_factor\n",
    "        # rescale weights\n",
    "        converted_weights[l] = converted_weights[l] / applied_factor\n",
    "\n",
    "        # rescale bias\n",
    "        if converted_bias is not None and converted_bias[l] is not None:\n",
    "            converted_bias[l] = converted_bias[l] / scale_factor\n",
    "        previous_factor = scale_factor\n",
    "        print(f\"Scale factor for this layer is {previous_factor}\")\n",
    "\n",
    "    return converted_weights, converted_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing weight conversion methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(\"analog_feedforward.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ = np.array([weights[0], weights[1]])\n",
    "# bias = np.array([weights[1], weights[3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "(ds_train, ds_test) = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True)\n",
    "ds_train = ds_train.map(flatten, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "result = get_activations_layer(model.input, model.layers[1].output, ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUB reduce error out of memory [Op:Max]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInternalError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-58f166a62663>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mconverted_weights_data_or_robust\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mweight_conversion_robust_and_data_based\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweights_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mds_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-26-b8beb5a59806>\u001B[0m in \u001B[0;36mweight_conversion_robust_and_data_based\u001B[0;34m(weights, bias, model, data, normalization_method, ppercentile)\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mmax_act\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpos_inputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mppercentile\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpos_inputs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0;31m# get the maximum weight in the layer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0mmax_wt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce_max\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconverted_weights\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mconverted_bias\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mconverted_bias\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0mmax_bias\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce_max\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconverted_bias\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mreduce_max\u001B[0;34m(input_tensor, axis, keepdims, name)\u001B[0m\n\u001B[1;32m   2682\u001B[0m     \u001B[0mThe\u001B[0m \u001B[0mreduced\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2683\u001B[0m   \"\"\"\n\u001B[0;32m-> 2684\u001B[0;31m   return reduce_max_with_dims(input_tensor, axis, keepdims, name,\n\u001B[0m\u001B[1;32m   2685\u001B[0m                               _ReductionDims(input_tensor, axis))\n\u001B[1;32m   2686\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mreduce_max_with_dims\u001B[0;34m(input_tensor, axis, keepdims, name, dims)\u001B[0m\n\u001B[1;32m   2694\u001B[0m   return _may_reduce_to_scalar(\n\u001B[1;32m   2695\u001B[0m       \u001B[0mkeepdims\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2696\u001B[0;31m       gen_math_ops._max(input_tensor, dims, keepdims, name=name))\n\u001B[0m\u001B[1;32m   2697\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2698\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36m_max\u001B[0;34m(input, axis, keep_dims, name)\u001B[0m\n\u001B[1;32m   5705\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5706\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5707\u001B[0;31m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5708\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5709\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   6841\u001B[0m   \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\" name: \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6842\u001B[0m   \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6843\u001B[0;31m   \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6844\u001B[0m   \u001B[0;31m# pylint: enable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6845\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/transformer-snn-conversion/lib/python3.8/site-packages/six.py\u001B[0m in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "\u001B[0;31mInternalError\u001B[0m: CUB reduce error out of memory [Op:Max]"
     ]
    }
   ],
   "source": [
    "converted_weights_data_or_robust = weight_conversion_robust_and_data_based(weights_, None, model, ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale factor for this layer is 0.9999994039535522\n",
      "Scale factor for this layer is 3.2180092334747314\n"
     ]
    }
   ],
   "source": [
    "converted_weights_model = weight_conversion_model(weights_, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying weights to the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_weights = weight_conversion(weights_)\n",
    "\n",
    "expanded_converted_wt_0 = tf.expand_dims(converted_weights[0], 0)\n",
    "expanded_converted_wt_1 = tf.expand_dims(converted_weights[1], 0)\n",
    "\n",
    "model.layers[1].set_weights(expanded_converted_wt_0)\n",
    "model.layers[2].set_weights(expanded_converted_wt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 1ms/step - loss: 2.3026 - sparse_categorical_accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3025832176208496, 0.09799999743700027]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_wt_0 = tf.expand_dims(weights[0], 0)\n",
    "expanded_wt_1 = tf.expand_dims(weights[2], 0)\n",
    "\n",
    "model.layers[1].set_weights(expanded_wt_0)\n",
    "model.layers[2].set_weights(expanded_wt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-570db8185f78>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.layers[1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 5ms/step - loss: 2.2662 - sparse_categorical_accuracy: 0.3068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.266212224960327, 0.3068000078201294]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_wt_0 = tf.random.normal(weights[0].shape)\n",
    "random_wt_0 = tf.expand_dims(random_wt_0, 0)\n",
    "\n",
    "random_wt_1 = tf.random.normal(weights[2].shape)\n",
    "random_wt_1 = tf.expand_dims(random_wt_1, 0)\n",
    "\n",
    "model.layers[1].set_weights(random_wt_0)\n",
    "model.layers[2].set_weights(random_wt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 6ms/step - loss: 13.5289 - sparse_categorical_accuracy: 0.1095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.528854370117188, 0.10949999839067459]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}