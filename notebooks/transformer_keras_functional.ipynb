{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return lang1, lang2\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_pt, result_en\n",
    "\n",
    "def filter_max_length(x, y, max_length=40):\n",
    "    return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)\n",
    "\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles( np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "d_model = 512\n",
    "dff=2048\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "# Size of input vocab plus start and end tokens\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "\n",
    "# Encoder ##################################\n",
    "inp = tf.keras.layers.Input(shape=(None,))\n",
    "x = tf.keras.layers.Embedding(input_vocab_size, d_model)(inp)\n",
    "\n",
    "## positional encoding\n",
    "scaling_factor = tf.keras.backend.constant(np.sqrt(d_model), shape = (1,1,1))\n",
    "x = tf.keras.layers.Multiply()([x,scaling_factor])\n",
    "pos = positional_encoding(maximum_position_encoding, d_model)\n",
    "x = tf.keras.layers.Add()([x, pos[: , :tf.shape(x)[1], :]] )\n",
    "\n",
    "## self-attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)\n",
    "value = tf.keras.layers.Dense(d_model)(x)\n",
    "key = tf.keras.layers.Dense(d_model)(x)\n",
    "attention = tf.keras.layers.Attention()([query, value, key])\n",
    "attention = tf.keras.layers.Dense(d_model)(attention)\n",
    "x = tf.keras.layers.Add()([x , attention]) # residual connection\n",
    "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "## Feed Forward\n",
    "dense = tf.keras.layers.Dense(dff, activation='relu')(x)\n",
    "dense = tf.keras.layers.Dense(d_model)(dense)\n",
    "x = tf.keras.layers.Add()([x , dense])     # residual connection\n",
    "encoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "\n",
    "# Decoder ##################################\n",
    "target = tf.keras.layers.Input(shape=(None,))\n",
    "x = tf.keras.layers.Embedding(target_vocab_size, d_model)(target)\n",
    "\n",
    "## positional encoding\n",
    "x = tf.keras.layers.Multiply()([x,scaling_factor])\n",
    "pos = positional_encoding(maximum_position_encoding, d_model)\n",
    "x = tf.keras.layers.Add()([x, pos[: , :tf.shape(x)[1], :] ])           \n",
    "\n",
    "## self-attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)\n",
    "value = tf.keras.layers.Dense(d_model)(x)\n",
    "key = tf.keras.layers.Dense(d_model)(x)\n",
    "attention = tf.keras.layers.Attention(causal = True)([query, value, key])\n",
    "attention = tf.keras.layers.Dense(d_model)(attention)\n",
    "x = tf.keras.layers.Add()([x , attention])  # residual connection\n",
    "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "## encoder-decoder attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)\n",
    "value = tf.keras.layers.Dense(d_model)(encoder)\n",
    "key = tf.keras.layers.Dense(d_model)(encoder)\n",
    "attention = tf.keras.layers.Attention()([query, value, key])\n",
    "attention = tf.keras.layers.Dense(d_model)(attention)\n",
    "x = tf.keras.layers.Add()([x , attention])  # residual connection\n",
    "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "## Feed Forward\n",
    "dense = tf.keras.layers.Dense(dff, activation='relu')(x)\n",
    "dense = tf.keras.layers.Dense(d_model)(dense)\n",
    "x = tf.keras.layers.Add()([x , dense])      # residual connection\n",
    "decoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "######################################################\n",
    "\n",
    "x = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
    "model = tf.keras.models.Model(inputs=[inp,target], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    _loss = loss(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=_loss.dtype)\n",
    "    _loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(_loss)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "metrics = [loss, masked_loss, tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss = loss, metrics = metrics) # masked_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 54s 1s/step - loss: 4.5435 - sparse_categorical_crossentropy: 4.5435 - masked_loss: 7.9058 - sparse_categorical_accuracy: 0.4862 - val_loss: 3.1426 - val_sparse_categorical_crossentropy: 3.1440 - val_masked_loss: 6.6800 - val_sparse_categorical_accuracy: 0.5642\n"
     ]
    }
   ],
   "source": [
    "def generator(data_set):\n",
    "    while True:\n",
    "        for pt_batch, en_batch in data_set:\n",
    "            yield ( [pt_batch , en_batch[:, :-1] ] , en_batch[:, 1:] )\n",
    "\n",
    "def training_map(pt, en):\n",
    "    return [pt , en[:-1]] , en[1:]\n",
    "\n",
    "history = model.fit(x = generator(train_dataset), validation_data = generator(val_dataset), epochs=1, steps_per_epoch = 50, validation_steps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did they eat fish and chips ?\n",
      "\n",
      "\n",
      "i was always worried about being caught and sent back .\n",
      "\n",
      "\n",
      "i chose one with the skin color of a lobster when sunburnt .\n",
      "\n",
      "\n",
      "but i think this is quite clearly untrue .\n",
      "\n",
      "\n",
      "we have measured our progress very rigorously .\n",
      "\n",
      "\n",
      "and from what i feel , it 's a cure for me , but for us all .\n",
      "\n",
      "\n",
      "it 's a work in progress from a personal story to a global history .\n",
      "\n",
      "\n",
      "i mean , it 's just a losing proposition .\n",
      "\n",
      "\n",
      "so , how do we have these conversations more easily and more often ?\n",
      "\n",
      "\n",
      "and issue rogue certificates .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "for i in range(10):\n",
    "    translation = [tokenizer_en.vocab_size]\n",
    "    for _ in range(40):\n",
    "        predict = model.predict([pt_batch[i:i+1],np.asarray([translation])])\n",
    "        translation.append(np.argmax(predict[-1,-1]))\n",
    "        if translation[-1] == tokenizer_en.vocab_size + 1:\n",
    "            break\n",
    "\n",
    "    real_translation = []\n",
    "    for w in en_batch[:, 1:][i].numpy():\n",
    "        if w == tokenizer_en.vocab_size + 1:\n",
    "            break\n",
    "        real_translation.append(w)\n",
    "    print(tokenizer_en.decode(real_translation))\n",
    "    print(tokenizer_en.decode(translation[1:-1]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(40):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input,\n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result\n",
    "                                            if i < tokenizer_en.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_masks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-9c87fa8c1259>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtranslate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"este é um problema que temos que resolver.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"Real translation: this is a problem we have to solve .\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-22-5226b9acb141>\u001B[0m in \u001B[0;36mtranslate\u001B[0;34m(sentence, plot)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mtranslate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m   \u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m   predicted_sentence = tokenizer_en.decode([i for i in result \n\u001B[1;32m      5\u001B[0m                                             if i < tokenizer_en.vocab_size])  \n",
      "\u001B[0;32m<ipython-input-21-86fb5e82b5dc>\u001B[0m in \u001B[0;36mevaluate\u001B[0;34m(inp_sentence)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m   \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m40\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n\u001B[0m\u001B[1;32m     16\u001B[0m         encoder_input, output)\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'create_masks' is not defined"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}